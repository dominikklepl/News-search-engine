{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Code explanation.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dominikklepl/News-search-engine/blob/master/Code_explanation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkJu3K5k8tWk",
        "colab_type": "text"
      },
      "source": [
        "# News search engine\n",
        "In this notebook we'll go through all code for building our own search engine specialised on news articles.\n",
        "Each search engine consists of three main components:\n",
        "\n",
        "\n",
        "* Crawler\n",
        "* Indexer\n",
        "* Query processor\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG-1-CN_mLTu",
        "colab_type": "text"
      },
      "source": [
        "## A. Crawler\n",
        "We start with building crawler that goes to RSS feed, extracts title, description (if any), date published and link. Processes this information and saves in a meta-data \"database\" (actually a pandas dataframe and stores it as csv file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfRHdlHTmLTx",
        "colab_type": "text"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKMcZm97mLTy",
        "colab_type": "code",
        "outputId": "91293aeb-1c75-45da-debf-bc9c1283bcd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install feedparser\n",
        "\n",
        "import feedparser\n",
        "import pandas as pd"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.6/dist-packages (5.2.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99djxLQM2GJL",
        "colab_type": "text"
      },
      "source": [
        "Read RSS feed from a URL. Also keep track of how many news are in the feed and how many of these have already been scraped with previous iteration of the crawler. This will be later important to automatically adjust the frequency with which the crawler will visit the RSS feed to scrape new information.\n",
        "For testing we use just one URL: BBC World News."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGjcps0s2rc4",
        "colab_type": "code",
        "outputId": "a1f4aa2a-5758-4e63-8817-26336188c563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "URL = \"http://feeds.bbci.co.uk/news/world/rss.xml\"\n",
        "feed = feedparser.parse(URL)\n",
        "\n",
        "feed_len = len(feed.entries) #number of news in feed\n",
        "old_news = 0  # count how many news in feed were already scraped\n",
        "\n",
        "print(\"There are {} news in the RSS feed.\" .format(feed_len))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 39 news in the RSS feed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27WxoGCj2vz3",
        "colab_type": "text"
      },
      "source": [
        "Load the meta-data database stored as csv file. If this is the first time the crawler is let loose this will be just an empty file with prepared column names (ID, title, date and link).\n",
        "I'll keep it commented out here and instead just create an empty dataframe a this point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6VVIE0W3FLm",
        "colab_type": "code",
        "outputId": "c312fde4-19d5-4bc1-9876-e734951df38a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        }
      },
      "source": [
        "#meta_data = pd.read_csv(PATH + \"database.csv\", index_col = 'Unnamed: 0')\n",
        "meta_data = pd.DataFrame(columns=['ID', 'title', 'summary', 'link', 'published'])\n",
        "meta_data.head()"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [ID, title, summary, link, published]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZrsrQxM4ADx",
        "colab_type": "text"
      },
      "source": [
        "Now we write a function that accepts one entry from the feed and parse its contents to list with title, date published and link and assigns it a unique ID (which also denotes when was the entry scraped and entered to our search engine).\n",
        "We normalize the date so that it's in format day/month/year. We don't care about more precise time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLs86-jb4Ygm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_entry(entry, ID):\n",
        "  ID = ID\n",
        "  title = entry.title\n",
        "  summary = entry.summary\n",
        "  link = entry.link\n",
        "  published = str(entry.published_parsed.tm_mday) + '/' + \\\n",
        "              str(entry.published_parsed.tm_mon) + '/' + \\\n",
        "              str(entry.published_parsed.tm_year)\n",
        "  return [ID, title, summary, link, published]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ6kXhpj4dPY",
        "colab_type": "text"
      },
      "source": [
        "Test the function on one entry"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6oLq8av4fSV",
        "colab_type": "code",
        "outputId": "96f42568-34ac-46e1-83ce-aa3b3b6a7cd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "test_entry = feed.entries[0]\n",
        "process_entry(test_entry, 1)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " \"Trump impeachment inquiry: Envoy 'intimidated' by tweets during testimony\",\n",
              " 'The president criticised the former US envoy to Ukraine in the middle of her impeachment testimony.',\n",
              " 'https://www.bbc.co.uk/news/world-us-canada-50436521',\n",
              " '15/11/2019']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMUwB0-u6xd3",
        "colab_type": "text"
      },
      "source": [
        "Now we iterate over all entries in the feed. Check if the entry is already in the meta-data database, if not the entry is processed, assigned an ID and appended to a list of entries that will be later added to the database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3_zLbxk7FQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [] #dataframe for saving the entries\n",
        "n = len(meta_data)+1 #ID value based on the highest ID value in database\n",
        "for i in range(len(feed.entries)):\n",
        "  entry = feed.entries[i]\n",
        "  \n",
        "  #check that link isn't in the database yet\n",
        "  if entry.link not in meta_data['link'].values:\n",
        "    processed = process_entry(entry = entry, ID=n)\n",
        "    data.append(processed)\n",
        "    n += 1 #increase the ID value\n",
        "  else: old_news += 1 #count already scraped entries"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbOf3R417oQJ",
        "colab_type": "text"
      },
      "source": [
        "If there was at least one newly scraped entry, we add it to the database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6mtKUT_7ypt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if len(data) > 0:\n",
        "  #transform data to pandas DataFrame\n",
        "  news_extracted = pd.DataFrame(data, columns=['ID', 'title', 'summary', 'link', 'published'])\n",
        "\n",
        "  #add new news to the database\n",
        "  meta_data = pd.concat([meta_data, news_extracted], axis = 0)\n",
        "\n",
        "  #write database to a csv file\n",
        "  #meta_data.to_csv(PATH + \"database.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRErvxw48RGs",
        "colab_type": "text"
      },
      "source": [
        "Look at the database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtZVgM-98Szw",
        "colab_type": "code",
        "outputId": "132b0814-f230-47b7-dbc8-4a4602e0f5e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "meta_data.head(5)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Trump impeachment inquiry: Envoy 'intimidated'...</td>\n",
              "      <td>The president criticised the former US envoy t...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-us-canada-504...</td>\n",
              "      <td>15/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Roger Stone: Trump ally convicted of lying to ...</td>\n",
              "      <td>The former adviser to President Donald Trump i...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-us-canada-504...</td>\n",
              "      <td>15/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Hong Kong protests: China condemns 'appalling'...</td>\n",
              "      <td>Hong Kong's Justice Secretary Teresa Cheng was...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-asia-china-50...</td>\n",
              "      <td>15/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Chile protests: Government bows to demands for...</td>\n",
              "      <td>After weeks of unrest in the country, Chile ha...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-latin-america...</td>\n",
              "      <td>15/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Inventor of the famed 'Sourtoe Cocktail' dies</td>\n",
              "      <td>Captain Dick Stevenson invented the alcoholic ...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-us-canada-503...</td>\n",
              "      <td>15/11/2019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ID  ...   published\n",
              "0  1  ...  15/11/2019\n",
              "1  2  ...  15/11/2019\n",
              "2  3  ...  15/11/2019\n",
              "3  4  ...  15/11/2019\n",
              "4  5  ...  15/11/2019\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8IWAaU19VQ-",
        "colab_type": "text"
      },
      "source": [
        "Get percentage of already-scraped entries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gub9dSMX9aVy",
        "colab_type": "code",
        "outputId": "7905ebb3-d331-4663-c8c8-41a7accb1b64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"{} % of entries were already scraped.\" .format((old_news/feed_len)*100))"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 % of entries were already scraped.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2yM7uUb3yqT",
        "colab_type": "text"
      },
      "source": [
        "## B. Indexer\n",
        "Second part of a search engine is an indexer. It's basically a smart storage of our news articles in which we can later easily retrieve relative articles given a search query.\n",
        "It parses the title and description of the news articles scraped by the crawler to single words. All these words make up the vocabulary of our index. Next step is to put the ID of the article in the posting lists of the words that the article contains. For example article called \"This happened today\" will be stored in posting lists of terms \"this\", \"happened\" and \"today\".\n",
        "Before creating the index we preprocess the text of the articles in order to get rid of useless information. We the text of accents and turn everything to lowercase. Next we perform lemmatization. This is slightly smarter version of stemming. Essentially, it's a word normalization, e.g. all nouns to singular, all verbs in present tense etc.\n",
        "\n",
        "Let's do it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeVsyIKnG6hY",
        "colab_type": "text"
      },
      "source": [
        "#### Importing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvVE1rsxG8He",
        "colab_type": "code",
        "outputId": "1980b06a-0ef8-4923-b160-56ef4192db38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import string"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTbe_OoM57p9",
        "colab_type": "text"
      },
      "source": [
        "We start with an empty dictionary as our index. As we scrape more articles later we will instead of starting with an empty index just update the already created index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqwBMsjv6bOr",
        "colab_type": "text"
      },
      "source": [
        "The index is organized as:\n",
        "```\n",
        "{\n",
        "  \"word1\": \\[ID1, ID2, ...],\n",
        "  \"word2\": \\[ID5, ID8, ...],\n",
        "  ...\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_tsSula7ONI",
        "colab_type": "text"
      },
      "source": [
        "We start with just a single article from our meta-data database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TBG-No068Cx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entry = meta_data.loc[0,:].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCSyZSzUBS9V",
        "colab_type": "code",
        "outputId": "d6913a7c-7bcd-4901-9ff9-31ed5314229a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "entry"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ID                                                           1\n",
              "title        Trump impeachment inquiry: Envoy 'intimidated'...\n",
              "summary      The president criticised the former US envoy t...\n",
              "link         https://www.bbc.co.uk/news/world-us-canada-504...\n",
              "published                                           15/11/2019\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWfZEQpG7oEk",
        "colab_type": "text"
      },
      "source": [
        "### Text preprocessing\n",
        "Turn title to lowercase, remove accents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nP_6t0iKh84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_string(text):\n",
        "  text = text.lower() #to lowercase\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation)) #strip punctuation\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvWZvb4s_e-L",
        "colab_type": "code",
        "outputId": "9690e0ca-9fcb-463d-f689-dd53217bcf00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "process_string(entry.title)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'trump impeachment inquiry envoy intimidated by tweets during testimony'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFXsuTQKGSU6",
        "colab_type": "text"
      },
      "source": [
        "Now, lemmatize, i.e. word normalization.\n",
        "\n",
        "This method requires some additional information about the words. We need to find the word category of each word, e.g. verb, noun etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtWfwUq5GriG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N8ZzKMLG0dV",
        "colab_type": "text"
      },
      "source": [
        "Test the function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGxXCIneHKZ7",
        "colab_type": "code",
        "outputId": "1b5b9798-8749-46f8-da41-683670d5336a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(\"Apple: {}\\n Run: {}\\n Happy: {}\" .format(get_wordnet_pos(\"apple\"), get_wordnet_pos(\"run\"), get_wordnet_pos(\"happy\")))"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple: n\n",
            " Run: v\n",
            " Happy: a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFBmqVwOIBi_",
        "colab_type": "text"
      },
      "source": [
        "We also need to remove stopwords, i.e. words with low informational value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKRZSEhCI_cG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop = stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpgU0fANJCtz",
        "colab_type": "text"
      },
      "source": [
        "Now we'll iterate over all words in text, lemmatize and return the transformed string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q1Vnrm0IYTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lem = WordNetLemmatizer()\n",
        "\n",
        "def stop_lemmatize(doc):\n",
        "    tokens = nltk.word_tokenize(doc)\n",
        "    tmp = \"\"\n",
        "    for w in tokens:\n",
        "        if w not in stop:\n",
        "            tmp += lem.lemmatize(w, get_wordnet_pos(w)) + \" \"\n",
        "    return tmp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E0kf2AOIyZT",
        "colab_type": "code",
        "outputId": "006b90ec-7894-4ebc-9024-a10e4f3f0aed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "stop_lemmatize(doc = entry.title)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Trump impeachment inquiry : Envoy 'intimidated ' tweet testimony \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HoG2PiXLzfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_string(text):\n",
        "  text = text.lower() #to lowercase\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation)) #strip punctuation\n",
        "  text = stop_lemmatize(text)\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIhgs5AjMIAQ",
        "colab_type": "code",
        "outputId": "57204413-692d-4013-9a6b-6650fc16502c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%time process_string(entry.title)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.04 ms, sys: 4 µs, total: 4.04 ms\n",
            "Wall time: 4.68 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'trump impeachment inquiry envoy intimidate tweet testimony '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5ONtU65MRV5",
        "colab_type": "text"
      },
      "source": [
        "Now we apply the process_string function to all titles and summaries in our database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wucZF2ONMYJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meta_processed = meta_data.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-QeP2F9NlqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_df(df):\n",
        "  df['title'] = df['title'].apply(process_string)\n",
        "  df['summary'] = df['summary'].apply(process_string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f_GpqAHMztr",
        "colab_type": "code",
        "outputId": "5f853bc7-55f5-4c89-f847-6eb5858be213",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%time transform_df(meta_processed)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 119 ms, sys: 6.96 ms, total: 126 ms\n",
            "Wall time: 132 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAs0QsWoM9IQ",
        "colab_type": "code",
        "outputId": "6e1d127d-7e4b-4845-92af-c6d7419f4224",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "meta_processed.head(5)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>trump impeachment inquiry envoy intimidate twe...</td>\n",
              "      <td>president criticise former u envoy ukraine mid...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-us-canada-504...</td>\n",
              "      <td>15/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>roger stone trump ally convict lie congress</td>\n",
              "      <td>former adviser president donald trump convict ...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-us-canada-504...</td>\n",
              "      <td>15/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>hong kong protest china condemns appal attack ...</td>\n",
              "      <td>hong kongs justice secretary teresa cheng surr...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-asia-china-50...</td>\n",
              "      <td>15/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>chile protest government bow demand referendum</td>\n",
              "      <td>week unrest country chile agree hold referendu...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-latin-america...</td>\n",
              "      <td>15/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>inventor famed sourtoe cocktail dy</td>\n",
              "      <td>captain dick stevenson invent alcoholic cockta...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-us-canada-503...</td>\n",
              "      <td>15/11/2019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ID  ...   published\n",
              "0  1  ...  15/11/2019\n",
              "1  2  ...  15/11/2019\n",
              "2  3  ...  15/11/2019\n",
              "3  4  ...  15/11/2019\n",
              "4  5  ...  15/11/2019\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-p1OltOONMV",
        "colab_type": "text"
      },
      "source": [
        "In practice, we won't be transforming the whole meta-data database since that would mean creating index from scratch after every crawler iteration. Instead we would use only subset of the database with only newly added articles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoeBvn0wPJCM",
        "colab_type": "text"
      },
      "source": [
        "Now we can iterate over all entries to create the index. We'll go step by step again before wrapping it all in one nice function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD-MeaW2W8qC",
        "colab_type": "text"
      },
      "source": [
        "Merge title and summary into one field and drop all columns except for ID as we don't need those anymore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa8k3MXlXDcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meta_processed['text'] = meta_processed['title'] + \" \" + meta_processed['summary']\n",
        "drop_cols = ['title', 'summary', 'published', 'link']\n",
        "meta_processed = meta_processed.drop(drop_cols, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIoqUWscY7-J",
        "colab_type": "code",
        "outputId": "d4724e0a-a045-4753-a8c8-b8864f1d78d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "meta_processed.head(5)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>trump impeachment inquiry envoy intimidate twe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>roger stone trump ally convict lie congress  f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>hong kong protest china condemns appal attack ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>chile protest government bow demand referendum...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>inventor famed sourtoe cocktail dy  captain di...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ID                                               text\n",
              "0  1  trump impeachment inquiry envoy intimidate twe...\n",
              "1  2  roger stone trump ally convict lie congress  f...\n",
              "2  3  hong kong protest china condemns appal attack ...\n",
              "3  4  chile protest government bow demand referendum...\n",
              "4  5  inventor famed sourtoe cocktail dy  captain di..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N240s8Nj8mRD",
        "colab_type": "text"
      },
      "source": [
        "Add this part to a transform_df function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiino7VO8o7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_df(df):\n",
        "  df = df\n",
        "  df['title'] = df['title'].apply(process_string)\n",
        "  df['summary'] = df['summary'].apply(process_string)\n",
        "  df['text'] = df['title'] + \" \" + df['summary']\n",
        "  drop_cols = ['title', 'summary', 'published', 'link']\n",
        "  df = df.drop(drop_cols, axis=1)\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srGNuGhDZo3b",
        "colab_type": "text"
      },
      "source": [
        "Now we'll build index with just one entry."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8_Uw87eZtHh",
        "colab_type": "code",
        "outputId": "d9e12801-ebd4-446d-dd0a-1dd097a8ae8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "entry = meta_processed.loc[0,:].copy()\n",
        "print(entry)\n",
        "index_test = {}"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ID                                                      1\n",
            "text    trump impeachment inquiry envoy intimidate twe...\n",
            "Name: 0, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAzw0E4cZ5f9",
        "colab_type": "text"
      },
      "source": [
        "Split the entry to single words and return list and save entry's ID as object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C1n-qMUZ0_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = entry.text.split()\n",
        "ID = entry.ID"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO0AQCZRalH9",
        "colab_type": "text"
      },
      "source": [
        "Each word in index' vocabulary is a dictionary key and has its own posting list with IDs. Let's construct one word vocabulary as example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHgo8mtSae96",
        "colab_type": "code",
        "outputId": "0bf259d0-5795-46f3-8e8f-80bc16b88ed1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word = words[0]\n",
        "sample = {word: [ID]}\n",
        "print(sample)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'trump': [1]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWWaf60bbEPf",
        "colab_type": "text"
      },
      "source": [
        "Now we iterate over all words and if they aren't in the vocabulary yet we add them. Also for each word we append the entry ID to the posting list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwRfN3uObCur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word in words:\n",
        "  if word in index_test.keys():\n",
        "    if ID not in index_test[word]:\n",
        "      index_test[word].append(ID)\n",
        "  else:\n",
        "    index_test[word] = [ID]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0wlU3SnctOf",
        "colab_type": "code",
        "outputId": "fd742a55-36e6-45b4-a5cf-55d81bdcfbda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(index_test)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'trump': [1], 'impeachment': [1], 'inquiry': [1], 'envoy': [1], 'intimidate': [1], 'tweet': [1], 'testimony': [1], 'president': [1], 'criticise': [1], 'former': [1], 'u': [1], 'ukraine': [1], 'middle': [1]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJYeIbiRdU7l",
        "colab_type": "text"
      },
      "source": [
        "Now this process can be repeated for all entries in the database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XO1iPcPdkRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def index_it(entry, index):\n",
        "  words = entry.text.split()\n",
        "  ID = entry.ID\n",
        "  for word in words:\n",
        "    if word in index.keys():\n",
        "      if ID not in index[word]:\n",
        "        index[word].append(ID)\n",
        "    else:\n",
        "      index[word] = [ID]\n",
        "  return index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL1z26mseByu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "81776140-d6d1-4974-dadc-570598901029"
      },
      "source": [
        "ind = index_it(entry=entry, index= {})\n",
        "print(ind)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'trump': [1], 'impeachment': [1], 'inquiry': [1], 'envoy': [1], 'intimidate': [1], 'tweet': [1], 'testimony': [1], 'president': [1], 'criticise': [1], 'former': [1], 'u': [1], 'ukraine': [1], 'middle': [1]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPNcdzIZI-On",
        "colab_type": "text"
      },
      "source": [
        "Again we can iterate over all entries in the database with scraped articles, process them append to index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju8eHusneNaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def index_all(df, index):\n",
        "  for i in range(len(df)):\n",
        "    entry = df.loc[i,:]\n",
        "    index = index_it(entry = entry, index = index)\n",
        "  return index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgoKdHiGevlu",
        "colab_type": "code",
        "outputId": "c045a0de-e6b0-44cd-b599-c3aa8dab3e44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "index = index_all(meta_processed, index = {})\n",
        "len(index)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "447"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqj9PXDtJLgG",
        "colab_type": "text"
      },
      "source": [
        "Finally we wrap everything in one nice function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RR7asUo8BX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_index(df, index):\n",
        "    to_add = transform_df(df)\n",
        "    index = index_all(df = to_add, index = index)\n",
        "    return index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpZyJuBp8CyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = build_index(df = meta_data, index = {})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoZv54Ey8slh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4f8b62a-29a9-4af7-e20e-a53f76a04a92"
      },
      "source": [
        "len(idx)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "447"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkmRha0xAPhd",
        "colab_type": "text"
      },
      "source": [
        "And for future use we save the index to json file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7bAh9pUARGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "with open('index.json', 'w') as fp:\n",
        "    json.dump(idx, fp, sort_keys=True, indent=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vbaEOXCAaET",
        "colab_type": "text"
      },
      "source": [
        "It can be of course opened again with following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beY4e7mCAbzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('index.json', 'r') as f:\n",
        "    data = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}