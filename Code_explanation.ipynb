{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Code explanation.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dominikklepl/News-search-engine/blob/master/Code_explanation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkJu3K5k8tWk",
        "colab_type": "text"
      },
      "source": [
        "# News search engine\n",
        "In this notebook we'll go through all code for building our own search engine specialised on news articles.\n",
        "Each search engine consists of three main components:\n",
        "\n",
        "\n",
        "* Crawler\n",
        "* Indexer\n",
        "* Query processor\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG-1-CN_mLTu",
        "colab_type": "text"
      },
      "source": [
        "## A. Crawler\n",
        "We start with building crawler that goes to RSS feed, extracts title, description (if any), date published and link. Processes this information and saves in a meta-data \"database\" (actually a pandas dataframe and stores it as csv file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfRHdlHTmLTx",
        "colab_type": "text"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKMcZm97mLTy",
        "colab_type": "code",
        "outputId": "7a87fbe8-fa59-4e2d-952e-7d1f1c06b108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install feedparser\n",
        "\n",
        "import feedparser\n",
        "import pandas as pd"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.6/dist-packages (5.2.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99djxLQM2GJL",
        "colab_type": "text"
      },
      "source": [
        "Read RSS feed from a URL. Also keep track of how many news are in the feed and how many of these have already been scraped with previous iteration of the crawler. This will be later important to automatically adjust the frequency with which the crawler will visit the RSS feed to scrape new information.\n",
        "For testing we use just one URL: BBC World News."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGjcps0s2rc4",
        "colab_type": "code",
        "outputId": "7a7cf042-bf80-455f-e410-130470ceda14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "URL = \"http://feeds.bbci.co.uk/news/world/rss.xml\"\n",
        "feed = feedparser.parse(URL)\n",
        "\n",
        "feed_len = len(feed.entries) #number of news in feed\n",
        "old_news = 0  # count how many news in feed were already scraped\n",
        "\n",
        "print(\"There are {} news in the RSS feed.\" .format(feed_len))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 32 news in the RSS feed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27WxoGCj2vz3",
        "colab_type": "text"
      },
      "source": [
        "Load the meta-data database stored as csv file. If this is the first time the crawler is let loose this will be just an empty file with prepared column names (ID, title, date and link).\n",
        "I'll keep it commented out here and instead just create an empty dataframe a this point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6VVIE0W3FLm",
        "colab_type": "code",
        "outputId": "7d5e164c-9b65-431c-ef84-308a23d762f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        }
      },
      "source": [
        "#meta_data = pd.read_csv(PATH + \"database.csv\", index_col = 'Unnamed: 0')\n",
        "meta_data = pd.DataFrame(columns=['ID', 'title', 'summary', 'link', 'published'])\n",
        "meta_data.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [ID, title, summary, link, published]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZrsrQxM4ADx",
        "colab_type": "text"
      },
      "source": [
        "Now we write a function that accepts one entry from the feed and parse its contents to list with title, date published and link and assigns it a unique ID (which also denotes when was the entry scraped and entered to our search engine).\n",
        "We normalize the date so that it's in format day/month/year. We don't care about more precise time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLs86-jb4Ygm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_entry(entry, ID):\n",
        "  ID = ID\n",
        "  title = entry.title\n",
        "  summary = entry.summary\n",
        "  link = entry.link\n",
        "  published = str(entry.published_parsed.tm_mday) + '/' + \\\n",
        "              str(entry.published_parsed.tm_mon) + '/' + \\\n",
        "              str(entry.published_parsed.tm_year)\n",
        "  return [ID, title, summary, link, published]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ6kXhpj4dPY",
        "colab_type": "text"
      },
      "source": [
        "Test the function on one entry"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6oLq8av4fSV",
        "colab_type": "code",
        "outputId": "c4230984-013a-49b4-a8b9-1012d29412bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "test_entry = feed.entries[0]\n",
        "process_entry(test_entry, 1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 'Iran petrol price hike: Protests erupt over surprise rationing',\n",
              " 'Several cities see protesters take to the streets as petrol prices go up by at least 50%.',\n",
              " 'https://www.bbc.co.uk/news/world-middle-east-50444429',\n",
              " '16/11/2019']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMUwB0-u6xd3",
        "colab_type": "text"
      },
      "source": [
        "Now we iterate over all entries in the feed. Check if the entry is already in the meta-data database, if not the entry is processed, assigned an ID and appended to a list of entries that will be later added to the database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3_zLbxk7FQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [] #dataframe for saving the entries\n",
        "n = len(meta_data)+1 #ID value based on the highest ID value in database\n",
        "for i in range(len(feed.entries)):\n",
        "  entry = feed.entries[i]\n",
        "  \n",
        "  #check that link isn't in the database yet\n",
        "  if entry.link not in meta_data['link'].values:\n",
        "    processed = process_entry(entry = entry, ID=n)\n",
        "    data.append(processed)\n",
        "    n += 1 #increase the ID value\n",
        "  else: old_news += 1 #count already scraped entries"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbOf3R417oQJ",
        "colab_type": "text"
      },
      "source": [
        "If there was at least one newly scraped entry, we add it to the database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6mtKUT_7ypt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if len(data) > 0:\n",
        "  #transform data to pandas DataFrame\n",
        "  news_extracted = pd.DataFrame(data, columns=['ID', 'title', 'summary', 'link', 'published'])\n",
        "\n",
        "  #add new news to the database\n",
        "  meta_data = pd.concat([meta_data, news_extracted], axis = 0)\n",
        "\n",
        "  #write database to a csv file\n",
        "  #meta_data.to_csv(PATH + \"database.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRErvxw48RGs",
        "colab_type": "text"
      },
      "source": [
        "Look at the database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtZVgM-98Szw",
        "colab_type": "code",
        "outputId": "275dfc7e-55be-48c2-da4a-b3208dc4360b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "meta_data.head(5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Iran petrol price hike: Protests erupt over su...</td>\n",
              "      <td>Several cities see protesters take to the stre...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-middle-east-5...</td>\n",
              "      <td>16/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Impeachment inquiry: Trump 'asked for probe in...</td>\n",
              "      <td>A US diplomatic aide says he overheard an envo...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-us-canada-504...</td>\n",
              "      <td>16/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Hong Kong protests: Chinese soldiers clean up ...</td>\n",
              "      <td>Chinese soldiers in Hong Kong have left their ...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-asia-china-50...</td>\n",
              "      <td>16/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Sri Lanka votes for president in shadow of Eas...</td>\n",
              "      <td>Terror fears could see the defence chief who o...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-asia-50013478</td>\n",
              "      <td>16/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Rodney Reed: Texas court halts execution in hi...</td>\n",
              "      <td>Rodney Reed had been set to die in days, but l...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-us-canada-504...</td>\n",
              "      <td>16/11/2019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ID  ...   published\n",
              "0  1  ...  16/11/2019\n",
              "1  2  ...  16/11/2019\n",
              "2  3  ...  16/11/2019\n",
              "3  4  ...  16/11/2019\n",
              "4  5  ...  16/11/2019\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8IWAaU19VQ-",
        "colab_type": "text"
      },
      "source": [
        "Get percentage of already-scraped entries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gub9dSMX9aVy",
        "colab_type": "code",
        "outputId": "39581bd7-15b3-4096-84e2-11f334f02b3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"{} % of entries were already scraped.\" .format((old_news/feed_len)*100))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 % of entries were already scraped.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2yM7uUb3yqT",
        "colab_type": "text"
      },
      "source": [
        "## B. Indexer\n",
        "Second part of a search engine is an indexer. It's basically a smart storage of our news articles in which we can later easily retrieve relative articles given a search query.\n",
        "It parses the title and description of the news articles scraped by the crawler to single words. All these words make up the vocabulary of our index. Next step is to put the ID of the article in the posting lists of the words that the article contains. For example article called \"This happened today\" will be stored in posting lists of terms \"this\", \"happened\" and \"today\".\n",
        "Before creating the index we preprocess the text of the articles in order to get rid of useless information. We the text of accents and turn everything to lowercase. Next we perform lemmatization. This is slightly smarter version of stemming. Essentially, it's a word normalization, e.g. all nouns to singular, all verbs in present tense etc.\n",
        "\n",
        "Let's do it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeVsyIKnG6hY",
        "colab_type": "text"
      },
      "source": [
        "#### Importing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvVE1rsxG8He",
        "colab_type": "code",
        "outputId": "eb81c436-c19a-4332-f205-b4bced56c626",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import string"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTbe_OoM57p9",
        "colab_type": "text"
      },
      "source": [
        "We start with an empty dictionary as our index. As we scrape more articles later we will instead of starting with an empty index just update the already created index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqwBMsjv6bOr",
        "colab_type": "text"
      },
      "source": [
        "The index is organized as:\n",
        "```\n",
        "{\n",
        "  \"word1\": \\[ID1, ID2, ...],\n",
        "  \"word2\": \\[ID5, ID8, ...],\n",
        "  ...\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_tsSula7ONI",
        "colab_type": "text"
      },
      "source": [
        "We start with just a single article from our meta-data database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TBG-No068Cx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entry = meta_data.loc[0,:].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCSyZSzUBS9V",
        "colab_type": "code",
        "outputId": "3fa9f447-7b5d-4e7f-bd37-97bc31200bcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "entry"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ID                                                           1\n",
              "title        Iran petrol price hike: Protests erupt over su...\n",
              "summary      Several cities see protesters take to the stre...\n",
              "link         https://www.bbc.co.uk/news/world-middle-east-5...\n",
              "published                                           16/11/2019\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWfZEQpG7oEk",
        "colab_type": "text"
      },
      "source": [
        "### Text preprocessing\n",
        "Turn title to lowercase, remove accents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nP_6t0iKh84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_string(text):\n",
        "  text = text.lower() #to lowercase\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation)) #strip punctuation\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvWZvb4s_e-L",
        "colab_type": "code",
        "outputId": "62f053ae-cb8b-404b-a705-c0dffe01fad6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "process_string(entry.title)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'iran petrol price hike protests erupt over surprise rationing'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFXsuTQKGSU6",
        "colab_type": "text"
      },
      "source": [
        "Now, lemmatize, i.e. word normalization.\n",
        "\n",
        "This method requires some additional information about the words. We need to find the word category of each word, e.g. verb, noun etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtWfwUq5GriG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N8ZzKMLG0dV",
        "colab_type": "text"
      },
      "source": [
        "Test the function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGxXCIneHKZ7",
        "colab_type": "code",
        "outputId": "77cd67bd-0c47-4fed-8042-256e77cebd8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print(\"Apple: {}\\n Run: {}\\n Happy: {}\" .format(get_wordnet_pos(\"apple\"), get_wordnet_pos(\"run\"), get_wordnet_pos(\"happy\")))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple: n\n",
            " Run: v\n",
            " Happy: a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFBmqVwOIBi_",
        "colab_type": "text"
      },
      "source": [
        "We also need to remove stopwords, i.e. words with low informational value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKRZSEhCI_cG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop = stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpgU0fANJCtz",
        "colab_type": "text"
      },
      "source": [
        "Now we'll iterate over all words in text, lemmatize and return the transformed string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q1Vnrm0IYTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lem = WordNetLemmatizer()\n",
        "\n",
        "def stop_lemmatize(doc):\n",
        "    tokens = nltk.word_tokenize(doc)\n",
        "    tmp = \"\"\n",
        "    for w in tokens:\n",
        "        if w not in stop:\n",
        "            tmp += lem.lemmatize(w, get_wordnet_pos(w)) + \" \"\n",
        "    return tmp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E0kf2AOIyZT",
        "colab_type": "code",
        "outputId": "ee2e41ab-b68b-4191-dedf-625c922f99b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "stop_lemmatize(doc = entry.title)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Iran petrol price hike : Protests erupt surprise ration '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HoG2PiXLzfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_string(text):\n",
        "  text = text.lower() #to lowercase\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation)) #strip punctuation\n",
        "  text = stop_lemmatize(text)\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIhgs5AjMIAQ",
        "colab_type": "code",
        "outputId": "3bdc20b4-9d55-4f35-c65a-3f3100813647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "%time process_string(entry.title)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3.42 ms, sys: 70 Âµs, total: 3.49 ms\n",
            "Wall time: 3.9 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'iran petrol price hike protest erupt surprise ration '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5ONtU65MRV5",
        "colab_type": "text"
      },
      "source": [
        "Now we apply the process_string function to all titles and summaries in our database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wucZF2ONMYJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meta_processed = meta_data.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-QeP2F9NlqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_df(df):\n",
        "  df['title'] = df['title'].apply(process_string)\n",
        "  df['summary'] = df['summary'].apply(process_string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f_GpqAHMztr",
        "colab_type": "code",
        "outputId": "ee9bbb90-2db7-485e-8ae2-b9cb060116b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "%time transform_df(meta_processed)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 140 ms, sys: 4.74 ms, total: 144 ms\n",
            "Wall time: 148 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAs0QsWoM9IQ",
        "colab_type": "code",
        "outputId": "8c36507c-f994-4b51-c64b-8dd40b6f9382",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "meta_processed.head(5)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>iran petrol price hike protest erupt surprise ...</td>\n",
              "      <td>several city see protester take street petrol ...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-middle-east-5...</td>\n",
              "      <td>16/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>impeachment inquiry trump ask probe ukraine en...</td>\n",
              "      <td>u diplomatic aide say overheard envoy phone ca...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-us-canada-504...</td>\n",
              "      <td>16/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>hong kong protest chinese soldier clean street</td>\n",
              "      <td>chinese soldier hong kong left barrack help di...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-asia-china-50...</td>\n",
              "      <td>16/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>sri lanka vote president shadow easter sunday ...</td>\n",
              "      <td>terror fear could see defence chief oversaw de...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-asia-50013478</td>\n",
              "      <td>16/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>rodney reed texas court halt execution highpro...</td>\n",
              "      <td>rodney reed set die day lawyer say new evidenc...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-us-canada-504...</td>\n",
              "      <td>16/11/2019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ID  ...   published\n",
              "0  1  ...  16/11/2019\n",
              "1  2  ...  16/11/2019\n",
              "2  3  ...  16/11/2019\n",
              "3  4  ...  16/11/2019\n",
              "4  5  ...  16/11/2019\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-p1OltOONMV",
        "colab_type": "text"
      },
      "source": [
        "In practice, we won't be transforming the whole meta-data database since that would mean creating index from scratch after every crawler iteration. Instead we would use only subset of the database with only newly added articles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoeBvn0wPJCM",
        "colab_type": "text"
      },
      "source": [
        "Now we can iterate over all entries to create the index. We'll go step by step again before wrapping it all in one nice function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD-MeaW2W8qC",
        "colab_type": "text"
      },
      "source": [
        "Merge title and summary into one field and drop all columns except for ID as we don't need those anymore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa8k3MXlXDcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meta_processed['text'] = meta_processed['title'] + \" \" + meta_processed['summary']\n",
        "drop_cols = ['title', 'summary', 'published', 'link']\n",
        "meta_processed = meta_processed.drop(drop_cols, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIoqUWscY7-J",
        "colab_type": "code",
        "outputId": "9f73bf63-8b1b-4d49-d33b-2de3c63a5cb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "meta_processed.head(5)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>iran petrol price hike protest erupt surprise ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>impeachment inquiry trump ask probe ukraine en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>hong kong protest chinese soldier clean street...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>sri lanka vote president shadow easter sunday ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>rodney reed texas court halt execution highpro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ID                                               text\n",
              "0  1  iran petrol price hike protest erupt surprise ...\n",
              "1  2  impeachment inquiry trump ask probe ukraine en...\n",
              "2  3  hong kong protest chinese soldier clean street...\n",
              "3  4  sri lanka vote president shadow easter sunday ...\n",
              "4  5  rodney reed texas court halt execution highpro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N240s8Nj8mRD",
        "colab_type": "text"
      },
      "source": [
        "Add this part to a transform_df function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiino7VO8o7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_df(df):\n",
        "  df = df\n",
        "  df['title'] = df['title'].apply(process_string)\n",
        "  df['summary'] = df['summary'].apply(process_string)\n",
        "  df['text'] = df['title'] + \" \" + df['summary']\n",
        "  drop_cols = ['title', 'summary', 'published', 'link']\n",
        "  df = df.drop(drop_cols, axis=1)\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UMJEydsAeur",
        "colab_type": "text"
      },
      "source": [
        "### Build index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srGNuGhDZo3b",
        "colab_type": "text"
      },
      "source": [
        "Now we'll build index with just one entry."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8_Uw87eZtHh",
        "colab_type": "code",
        "outputId": "26592da5-c6ab-418f-e005-448f179a5dbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "entry = meta_processed.loc[0,:].copy()\n",
        "print(entry)\n",
        "index_test = {}"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ID                                                      1\n",
            "text    iran petrol price hike protest erupt surprise ...\n",
            "Name: 0, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAzw0E4cZ5f9",
        "colab_type": "text"
      },
      "source": [
        "Split the entry to single words and return list and save entry's ID as object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C1n-qMUZ0_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = entry.text.split()\n",
        "ID = entry.ID"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO0AQCZRalH9",
        "colab_type": "text"
      },
      "source": [
        "Each word in index' vocabulary is a dictionary key and has its own posting list with IDs. Let's construct one word vocabulary as example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHgo8mtSae96",
        "colab_type": "code",
        "outputId": "fd11ac71-a068-4b5e-d712-0ebb1aa8eb6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word = words[0]\n",
        "sample = {word: [ID]}\n",
        "print(sample)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'iran': [1]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWWaf60bbEPf",
        "colab_type": "text"
      },
      "source": [
        "Now we iterate over all words and if they aren't in the vocabulary yet we add them. Also for each word we append the entry ID to the posting list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwRfN3uObCur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word in words:\n",
        "  if word in index_test.keys():\n",
        "    if ID not in index_test[word]:\n",
        "      index_test[word].append(ID)\n",
        "  else:\n",
        "    index_test[word] = [ID]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0wlU3SnctOf",
        "colab_type": "code",
        "outputId": "b4df0a5d-eccf-4a24-c651-eda1ea6038bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(index_test)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'iran': [1], 'petrol': [1], 'price': [1], 'hike': [1], 'protest': [1], 'erupt': [1], 'surprise': [1], 'ration': [1], 'several': [1], 'city': [1], 'see': [1], 'protester': [1], 'take': [1], 'street': [1], 'go': [1], 'least': [1], '50': [1]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJYeIbiRdU7l",
        "colab_type": "text"
      },
      "source": [
        "Now this process can be repeated for all entries in the database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XO1iPcPdkRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def index_it(entry, index):\n",
        "  words = entry.text.split()\n",
        "  ID = entry.ID\n",
        "  for word in words:\n",
        "    if word in index.keys():\n",
        "      if ID not in index[word]:\n",
        "        index[word].append(ID)\n",
        "    else:\n",
        "      index[word] = [ID]\n",
        "  return index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL1z26mseByu",
        "colab_type": "code",
        "outputId": "e4c295f7-d2ce-4901-9cd8-576275822300",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "ind = index_it(entry=entry, index= {})\n",
        "print(ind)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'iran': [1], 'petrol': [1], 'price': [1], 'hike': [1], 'protest': [1], 'erupt': [1], 'surprise': [1], 'ration': [1], 'several': [1], 'city': [1], 'see': [1], 'protester': [1], 'take': [1], 'street': [1], 'go': [1], 'least': [1], '50': [1]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPNcdzIZI-On",
        "colab_type": "text"
      },
      "source": [
        "Again we can iterate over all entries in the database with scraped articles, process them append to index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju8eHusneNaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def index_all(df, index):\n",
        "  for i in range(len(df)):\n",
        "    entry = df.loc[i,:]\n",
        "    index = index_it(entry = entry, index = index)\n",
        "  return index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgoKdHiGevlu",
        "colab_type": "code",
        "outputId": "b911106f-bde0-46a2-c5ed-2658213034b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "index = index_all(meta_processed, index = {})\n",
        "len(index)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "364"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqj9PXDtJLgG",
        "colab_type": "text"
      },
      "source": [
        "Finally we wrap everything in one nice function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RR7asUo8BX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_index(df, index):\n",
        "    to_add = transform_df(df)\n",
        "    index = index_all(df = to_add, index = index)\n",
        "    return index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpZyJuBp8CyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = build_index(df = meta_data, index = {})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoZv54Ey8slh",
        "colab_type": "code",
        "outputId": "29337cb7-3b77-42ab-b195-bd1df4b11870",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(idx)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "364"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkmRha0xAPhd",
        "colab_type": "text"
      },
      "source": [
        "And for future use we save the index to json file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7bAh9pUARGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "with open('index.json', 'w') as fp:\n",
        "    json.dump(idx, fp, sort_keys=True, indent=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vbaEOXCAaET",
        "colab_type": "text"
      },
      "source": [
        "It can be of course opened again with following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beY4e7mCAbzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('index.json', 'r') as f:\n",
        "    data = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVI6tcJh9mzP",
        "colab_type": "text"
      },
      "source": [
        "# Bb Ranked retrieval\n",
        "  The user would probably prefer the more relevant pages to be displayed before those that are less relevant (hopefully they're at least a bit relevant).\n",
        "For our search engine to support such option we need to store some information about the scraped documents that could be later used for this purpose.\n",
        "We'll use averaged word2vec for this purpose. Word2Vec model is single hidden-layer neural network. The hidden layer is actually what is so useful about this model. Given a word the layer's activation gives a unique vector that word. For each document we can iterate over all words, extract their vectors and then by averaging obtain a document vector. \n",
        "  Compared to other methods averaged word2vec has multiple advantages. Unlike simpler methods such as bag-of-words, n-grams and tf-idf the size of the vectors is fixed. For example bag-of-words is also using vectors but the size of these vectors equals the number of unique words in the corpus. This means that the computational and storage requirements get larger as the corpus gets larger.\n",
        "  Averaged word2vec is also able to represent the documents on more abstract level than simpler methods and should therefore provide better method of ranking.\n",
        "  We're using word2vec rather than doc2vec because we can simply use pretrained word2vec model to compute the document vectors. Using doc2vec would mean training a neural network from scratch which requires computational power, time and rather large dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aogzj51-XIr",
        "colab_type": "text"
      },
      "source": [
        "Import and download pretrained word2vec model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzGvdfpM9rAr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "79de2542-d247-4de4-c9d9-b3937abe6b28"
      },
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-16 16:06:28--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.98.133\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.98.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: âGoogleNews-vectors-negative300.bin.gzâ\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  45.3MB/s    in 30s     \n",
            "\n",
            "2019-11-16 16:06:58 (52.9 MB/s) - âGoogleNews-vectors-negative300.bin.gzâ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5ICnU0Q-bfo",
        "colab_type": "text"
      },
      "source": [
        "Load word2vec model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYt1PwiB99RY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a60b468e-839d-498b-c1e1-7714f3ba6a05"
      },
      "source": [
        "word2vec = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQuWTsqA-fXc",
        "colab_type": "text"
      },
      "source": [
        "Try getting vectors for all words in the text and averaging to get single vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI5z0s2u-z-B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e3b3bca8-53fd-405b-9690-0020c63f5763"
      },
      "source": [
        "print(words)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['iran', 'petrol', 'price', 'hike', 'protest', 'erupt', 'surprise', 'ration', 'several', 'city', 'see', 'protester', 'take', 'street', 'petrol', 'price', 'go', 'least', '50']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIQbeAyZ-2ra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def average_vectors(word2vec_model, doc):\n",
        "    # remove out-of-vocabulary words\n",
        "    doc = [word for word in doc if word in word2vec_model.vocab]\n",
        "    if len(doc) == 0:\n",
        "      return np.zeros(300)\n",
        "    else:\n",
        "      return np.mean(word2vec_model[doc], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbVzj05u_BBz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2b3f49fb-75d7-4250-d380-35cc8dc252ea"
      },
      "source": [
        "%time test_vec = average_vectors(word2vec, words)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.25 ms, sys: 13 Âµs, total: 1.26 ms\n",
            "Wall time: 1.44 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSg9chdM_sMc",
        "colab_type": "text"
      },
      "source": [
        "Now we can iterate over documents, compute their vectors and construct a document vectors database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDajpm9W_z0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_ranking(df):\n",
        "  corpus = df[['ID', 'text']].copy()\n",
        "  doc_vecs = {}\n",
        "  for i in range(len(corpus)):\n",
        "    row = corpus.loc[i,:]\n",
        "    text = row.text.split()\n",
        "    doc_vecs[row.ID]=average_vectors(word2vec, text)\n",
        "  return doc_vecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdV6EV7EB0Ie",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "74b7a4d2-fd9a-4d94-88fd-36783fb7d778"
      },
      "source": [
        "doc_vecs = prepare_ranking(df=meta_data)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 20.4 ms, sys: 3.98 ms, total: 24.4 ms\n",
            "Wall time: 32.1 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i48TY7RwFMcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwLh0z5hAiGF",
        "colab_type": "text"
      },
      "source": [
        "## C. Query processor\n",
        "The final part of a search engine is a query processor which actually performs the search task. Given a query by user the processor should return list of relevant documents.\n",
        "There are multiple types of queries. We'll start with a simple \"google-ish\" query where assume the user looks for documents relevant to all words in the query. Therefore we transform the query to boolean by connecting all words with AND operator.\n",
        "\n",
        "First, the processor preprocesses the query the same way as the indexer preprocessed the text. In other words, we normalize the query to match the format of text in the index. Next, the query is parsed to single words. We look into index if these words are part of the vocabulary. If a word is in index we retrieve its posting list. Finally, we look for intersection of all retrieved posting lists. The result is list of document IDs that the user asked for.\n",
        "However, we need to return something more useful than just a list of IDs. Therefore,we retrieve the information stored about the documents in the meta-data database. Before printing the results we should also rank the documents. This ranking should be based on relevance to query.\n",
        "Optionally, the user may ask for news only from limited time window, e.g. published today or last week. So we need to filter the retrieved documents if this happens.\n",
        "\n",
        "-----\n",
        "To implement:\n",
        " - Boolean query\n",
        " - phrase matching\n",
        " -----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyNAejSKMEyx",
        "colab_type": "text"
      },
      "source": [
        "### Normalize query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPu2TdudD797",
        "colab_type": "text"
      },
      "source": [
        "Let's define an example query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JUyKhZvEKjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = \"Trump Ukraine China\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmMKVw28EXcW",
        "colab_type": "text"
      },
      "source": [
        "Now we use the \"process string\" function from used by indexer to normalize the query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZKoWbSvEWnp",
        "colab_type": "code",
        "outputId": "b55c6a75-55d3-4750-c76f-8e8d1d906e16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(\"User query: {}.\" .format(test))\n",
        "test_norm = process_string(test)\n",
        "print(\"Normalized query: {}.\" .format(test_norm))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User query: Trump Ukraine China.\n",
            "Normalized query: trump ukraine china .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg4x9bo6E2Nd",
        "colab_type": "text"
      },
      "source": [
        "Now we split the query into words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNL9FbheEiSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_split = test_norm.split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psMVdYaeFvSr",
        "colab_type": "text"
      },
      "source": [
        "And we wrap this in function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlFn4ChDF1Mh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_query(query):\n",
        "  norm = process_string(query)\n",
        "  return norm.split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UX4rtmWMKLX",
        "colab_type": "text"
      },
      "source": [
        "### Retrieve from index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3rrUZ4uFOZR",
        "colab_type": "text"
      },
      "source": [
        "And we iterate over the words, looking if they're in the index vocabulary. If so then we retrieve the associated posting list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceY2E-MzFLKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "retrieved = []\n",
        "for word in test_split:\n",
        "  if word in index.keys():\n",
        "    retrieved.append(index[word])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SEe_66IHyyk",
        "colab_type": "text"
      },
      "source": [
        "Now we look for the intersection of all posting lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0czw-uJUH2HD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lists_intersection(lists):\n",
        "  intersect = list(set.intersection(*map(set, lists)))\n",
        "  intersect.sort()\n",
        "  return intersect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckV3JFxxHEQv",
        "colab_type": "code",
        "outputId": "7a97365b-e6e1-405e-dbc6-ec659cc2f704",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "result = lists_intersection(retrieved)\n",
        "print(result)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 15, 16, 23]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNF2rAIvJjkk",
        "colab_type": "text"
      },
      "source": [
        "Let's wrap this part in a function before proceeding to formatting the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CQ4fLj9JqBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def retrieve_common(query_split, index):\n",
        "  retrieved = []\n",
        "  for word in query_split:\n",
        "    if word in index.keys():\n",
        "      retrieved.append(index[word])\n",
        "  result = lists_intersection(retrieved)\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6jorxcKK0NQ",
        "colab_type": "code",
        "outputId": "9d328242-c7c7-40bf-ec57-10a7e6b71605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "result_IDs = retrieve_common(test_split, index)\n",
        "print(result_IDs)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 15, 16, 23]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2bZhASYLQMT",
        "colab_type": "text"
      },
      "source": [
        "-----\n",
        "\n",
        "*TO DO:\n",
        "If there's no document retrieved, try removing one term and looking for simplified query + tell user that such document doesn't include term X.*\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu8le1C1MCWi",
        "colab_type": "text"
      },
      "source": [
        "### Retrieve meta-data\n",
        "Now we need to connect the retrieved IDs with some useful information stored in database that we first use to refine the results and then to print nice result to user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeNE1jlWMdsz",
        "colab_type": "code",
        "outputId": "9a1f283e-38db-4d35-97eb-4d80017b4567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "#in real setting we'll read the database from file here\n",
        "#meta = pd.read_csv(\"database.csv\")\n",
        "\n",
        "#this is our database\n",
        "meta = meta_data.drop(['text'], axis=1).copy()\n",
        "meta.head(5)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>iran petrol price hike protest erupt surprise ...</td>\n",
              "      <td>several city see protester take street petrol ...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-middle-east-5...</td>\n",
              "      <td>16/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>impeachment inquiry trump ask probe ukraine en...</td>\n",
              "      <td>u diplomatic aide say overheard envoy phone ca...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-us-canada-504...</td>\n",
              "      <td>16/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>hong kong protest chinese soldier clean street</td>\n",
              "      <td>chinese soldier hong kong left barrack help di...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-asia-china-50...</td>\n",
              "      <td>16/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>sri lanka vote president shadow easter sunday ...</td>\n",
              "      <td>terror fear could see defence chief oversaw de...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-asia-50013478</td>\n",
              "      <td>16/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>rodney reed texas court halt execution highpro...</td>\n",
              "      <td>rodney reed set die day lawyer say new evidenc...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-us-canada-504...</td>\n",
              "      <td>16/11/2019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ID  ...   published\n",
              "0  1  ...  16/11/2019\n",
              "1  2  ...  16/11/2019\n",
              "2  3  ...  16/11/2019\n",
              "3  4  ...  16/11/2019\n",
              "4  5  ...  16/11/2019\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C8jU3h7Nf3L",
        "colab_type": "text"
      },
      "source": [
        "Query from database to get only rows of retrieved IDs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAAm0Nc-M5Bh",
        "colab_type": "code",
        "outputId": "ee051c59-fc9a-40db-cd93-15234778ec58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        }
      },
      "source": [
        "result_meta = meta[meta.ID.isin(result_IDs)].reset_index(drop=True)\n",
        "result_meta.head(5)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>impeachment inquiry trump ask probe ukraine en...</td>\n",
              "      <td>u diplomatic aide say overheard envoy phone ca...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-us-canada-504...</td>\n",
              "      <td>16/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15</td>\n",
              "      <td>trump impeachment inquiry ukraine important u</td>\n",
              "      <td>relation eastern european country become centr...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-europe-50419668</td>\n",
              "      <td>15/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16</td>\n",
              "      <td>trump impeachment who ukraine story</td>\n",
              "      <td>story kickstarted impeachment move donald trum...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-us-canada-498...</td>\n",
              "      <td>7/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>23</td>\n",
              "      <td>trump livetweets attack yovanovitch interrupt ...</td>\n",
              "      <td>former ukraine ambassador responds president t...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-us-canada-504...</td>\n",
              "      <td>15/11/2019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID  ...   published\n",
              "0   2  ...  16/11/2019\n",
              "1  15  ...  15/11/2019\n",
              "2  16  ...   7/11/2019\n",
              "3  23  ...  15/11/2019\n",
              "\n",
              "[4 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0p1E-AgQymE",
        "colab_type": "text"
      },
      "source": [
        "### Date filtering\n",
        "User might ask for news from specific day or date range.\n",
        "For simplicity let's assume the user enters date in format day/month/year. \n",
        "Now we can define 2 types of date restrictions:\n",
        "* single day\n",
        "* date range - from X/X/X - to X/X/X\n",
        "\n",
        "Another option of date restriction could be saying 'today'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewBQDK20Vr_r",
        "colab_type": "text"
      },
      "source": [
        "Restricing to single day"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms7vreKCVw-O",
        "colab_type": "code",
        "outputId": "3711acb8-4940-4a35-9466-c3c631213bd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "test = \"16/11/2019\"\n",
        "\n",
        "#get news published on \"test\"\n",
        "results_single = result_meta[result_meta.published==test].reset_index(drop=True)\n",
        "results_single.head()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>impeachment inquiry trump ask probe ukraine en...</td>\n",
              "      <td>u diplomatic aide say overheard envoy phone ca...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-us-canada-504...</td>\n",
              "      <td>16/11/2019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ID  ...   published\n",
              "0  2  ...  16/11/2019\n",
              "\n",
              "[1 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8menLWWZWYkJ",
        "colab_type": "text"
      },
      "source": [
        "Restricing to \"today\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgQDX7UGWM5S",
        "colab_type": "code",
        "outputId": "1386f50d-88cb-426d-ba83-0b7f60e0d173",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "#get today's date\n",
        "from datetime import date, timedelta\n",
        "\n",
        "def get_today():\n",
        "  today = date.today()\n",
        "  today = today.strftime(\"%d/%m/%Y\")\n",
        "  return today\n",
        "\n",
        "results_today = result_meta[result_meta.published==get_today()].reset_index(drop=True)\n",
        "results_today.head()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>impeachment inquiry trump ask probe ukraine en...</td>\n",
              "      <td>u diplomatic aide say overheard envoy phone ca...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-us-canada-504...</td>\n",
              "      <td>16/11/2019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ID  ...   published\n",
              "0  2  ...  16/11/2019\n",
              "\n",
              "[1 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-2VaQl2W_tW",
        "colab_type": "text"
      },
      "source": [
        "Restricting to time interval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkDZvU5QWyoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def daterange(start, end):\n",
        "    for n in range(int ((end - start).days)+1):\n",
        "        yield start + timedelta(n)\n",
        "\n",
        "def format_date(dt):\n",
        "  dt = dt.split(\"/\")\n",
        "  dt = date(int(dt[2]), int(dt[1]), int(dt[0]))\n",
        "  return(dt)\n",
        "\n",
        "def date_interval(interval):\n",
        "  interval = interval.split(\"-\")\n",
        "  start = format_date(interval[0])\n",
        "  end = format_date(interval[1])\n",
        "  interval = []\n",
        "  for dt in daterange(start, end):\n",
        "      interval.append(dt.strftime(\"%d/%m/%Y\"))\n",
        "  return interval"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0x2ZHXJXlYD",
        "colab_type": "code",
        "outputId": "3790ee59-6240-4d80-dae0-64df0938abc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "date_interval(\"15/11/2019 - 16/11/2019\")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['15/11/2019', '16/11/2019']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-zOxFRfX5j3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}