{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Code explanation.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dominikklepl/News-search-engine/blob/master/Code_explanation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkJu3K5k8tWk",
        "colab_type": "text"
      },
      "source": [
        "# News search engine\n",
        "In this notebook we'll go through all code for building our own search engine specialised on news articles.\n",
        "Each search engine consists of three main components:\n",
        "\n",
        "\n",
        "* Crawler\n",
        "* Indexer\n",
        "* Query processor\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG-1-CN_mLTu",
        "colab_type": "text"
      },
      "source": [
        "## A. Crawler\n",
        "We start with building crawler that goes to RSS feed, extracts title, description (if any), date published and link. Processes this information and saves in a meta-data \"database\" (actually a pandas dataframe and stores it as csv file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfRHdlHTmLTx",
        "colab_type": "text"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKMcZm97mLTy",
        "colab_type": "code",
        "outputId": "9ea85215-15aa-44f2-ef4b-1e85079c9cc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "!pip install feedparser\n",
        "\n",
        "import feedparser\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting feedparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 16.2MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 30kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 40kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 51kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 61kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 71kB 10.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 81kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 92kB 12.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 102kB 10.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 112kB 10.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 122kB 10.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 133kB 10.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 143kB 10.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 153kB 10.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 163kB 10.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 174kB 10.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 184kB 10.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 10.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: feedparser\n",
            "  Building wheel for feedparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedparser: filename=feedparser-5.2.1-cp36-none-any.whl size=44940 sha256=baa1516fef5a91ceef4a7192d36d52a9e3a34d58a04487cf2ab50014918f860e\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
            "Successfully built feedparser\n",
            "Installing collected packages: feedparser\n",
            "Successfully installed feedparser-5.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99djxLQM2GJL",
        "colab_type": "text"
      },
      "source": [
        "Read RSS feed from a URL. Also keep track of how many news are in the feed and how many of these have already been scraped with previous iteration of the crawler. This will be later important to automatically adjust the frequency with which the crawler will visit the RSS feed to scrape new information.\n",
        "For testing we use just one URL: BBC World News."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGjcps0s2rc4",
        "colab_type": "code",
        "outputId": "8f82eea5-34f9-44d7-e60e-16e9613cafb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "URL = \"http://feeds.bbci.co.uk/news/world/rss.xml\"\n",
        "feed = feedparser.parse(URL)\n",
        "\n",
        "feed_len = len(feed.entries) #number of news in feed\n",
        "old_news = 0  # count how many news in feed were already scraped\n",
        "\n",
        "print(\"There are {} news in the RSS feed.\" .format(feed_len))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 28 news in the RSS feed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27WxoGCj2vz3",
        "colab_type": "text"
      },
      "source": [
        "Load the meta-data database stored as csv file. If this is the first time the crawler is let loose this will be just an empty file with prepared column names (ID, title, date and link).\n",
        "I'll keep it commented out here and instead just create an empty dataframe a this point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6VVIE0W3FLm",
        "colab_type": "code",
        "outputId": "e98f0a20-9378-40e4-8b90-258c9918e386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        }
      },
      "source": [
        "#meta_data = pd.read_csv(PATH + \"database.csv\", index_col = 'Unnamed: 0')\n",
        "meta_data = pd.DataFrame(columns=['ID', 'title', 'summary', 'link', 'published'])\n",
        "meta_data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [ID, title, summary, link, published]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZrsrQxM4ADx",
        "colab_type": "text"
      },
      "source": [
        "Now we write a function that accepts one entry from the feed and parse its contents to list with title, date published and link and assigns it a unique ID (which also denotes when was the entry scraped and entered to our search engine).\n",
        "We normalize the date so that it's in format day/month/year. We don't care about more precise time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLs86-jb4Ygm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_entry(entry, ID):\n",
        "  ID = ID\n",
        "  title = entry.title\n",
        "  summary = entry.summary\n",
        "  link = entry.link\n",
        "  published = str(entry.published_parsed.tm_mday) + '/' + \\\n",
        "              str(entry.published_parsed.tm_mon) + '/' + \\\n",
        "              str(entry.published_parsed.tm_year)\n",
        "  return [ID, title, summary, link, published]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ6kXhpj4dPY",
        "colab_type": "text"
      },
      "source": [
        "Test the function on one entry"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6oLq8av4fSV",
        "colab_type": "code",
        "outputId": "90f04c02-a734-4f8c-c83a-a8c3a8cffe9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "test_entry = feed.entries[0]\n",
        "process_entry(test_entry, 1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " \"Data leak reveals how China 'brainwashes' Uighurs in prison camps\",\n",
              " \"Leaked documents show new evidence of China's systematic brainwashing of Uighur and other detainees.\",\n",
              " 'https://www.bbc.co.uk/news/world-asia-china-50511063',\n",
              " '24/11/2019']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMUwB0-u6xd3",
        "colab_type": "text"
      },
      "source": [
        "Now we iterate over all entries in the feed. Check if the entry is already in the meta-data database, if not the entry is processed, assigned an ID and appended to a list of entries that will be later added to the database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3_zLbxk7FQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [] #dataframe for saving the entries\n",
        "n = len(meta_data)+1 #ID value based on the highest ID value in database\n",
        "for i in range(len(feed.entries)):\n",
        "  entry = feed.entries[i]\n",
        "  \n",
        "  #check that link isn't in the database yet\n",
        "  if entry.link not in meta_data['link'].values:\n",
        "    processed = process_entry(entry = entry, ID=n)\n",
        "    data.append(processed)\n",
        "    n += 1 #increase the ID value\n",
        "  else: old_news += 1 #count already scraped entries"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbOf3R417oQJ",
        "colab_type": "text"
      },
      "source": [
        "If there was at least one newly scraped entry, we add it to the database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6mtKUT_7ypt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if len(data) > 0:\n",
        "  #transform data to pandas DataFrame\n",
        "  news_extracted = pd.DataFrame(data, columns=['ID', 'title', 'summary', 'link', 'published'])\n",
        "\n",
        "  #add new news to the database\n",
        "  meta_data = pd.concat([meta_data, news_extracted], axis = 0)\n",
        "\n",
        "  #write database to a csv file\n",
        "  #meta_data.to_csv(PATH + \"database.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRErvxw48RGs",
        "colab_type": "text"
      },
      "source": [
        "Look at the database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtZVgM-98Szw",
        "colab_type": "code",
        "outputId": "ff13e85f-0023-485a-f6d6-1b6de3be054f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "meta_data.head(5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Data leak reveals how China 'brainwashes' Uigh...</td>\n",
              "      <td>Leaked documents show new evidence of China's ...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-asia-china-50...</td>\n",
              "      <td>24/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Hong Kong elections: Pro-democracy group 'make...</td>\n",
              "      <td>Opposition candidates have attracted support a...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-asia-china-50...</td>\n",
              "      <td>24/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>DR Congo: Many dead as plane crashes into homes</td>\n",
              "      <td>At least 27 people are dead after a passenger ...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-africa-50536220</td>\n",
              "      <td>24/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Floods cause chaos in France and Italy</td>\n",
              "      <td>A bridge collapsed near the Italian city of Sa...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-europe-50536502</td>\n",
              "      <td>24/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Queen Hind cargo ship carrying 14,000 sheep ov...</td>\n",
              "      <td>An operation to rescue the animals is under wa...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-europe-50538592</td>\n",
              "      <td>24/11/2019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ID  ...   published\n",
              "0  1  ...  24/11/2019\n",
              "1  2  ...  24/11/2019\n",
              "2  3  ...  24/11/2019\n",
              "3  4  ...  24/11/2019\n",
              "4  5  ...  24/11/2019\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8IWAaU19VQ-",
        "colab_type": "text"
      },
      "source": [
        "Get percentage of already-scraped entries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gub9dSMX9aVy",
        "colab_type": "code",
        "outputId": "8ed7e268-fa1c-46ef-f524-b2d803f5bc32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"{} % of entries were already scraped.\" .format((old_news/feed_len)*100))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 % of entries were already scraped.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2yM7uUb3yqT",
        "colab_type": "text"
      },
      "source": [
        "## B. Indexer\n",
        "Second part of a search engine is an indexer. It's basically a smart storage of our news articles in which we can later easily retrieve relative articles given a search query.\n",
        "It parses the title and description of the news articles scraped by the crawler to single words. All these words make up the vocabulary of our index. Next step is to put the ID of the article in the posting lists of the words that the article contains. For example article called \"This happened today\" will be stored in posting lists of terms \"this\", \"happened\" and \"today\".\n",
        "Before creating the index we preprocess the text of the articles in order to get rid of useless information. We the text of accents and turn everything to lowercase. Next we perform lemmatization. This is slightly smarter version of stemming. Essentially, it's a word normalization, e.g. all nouns to singular, all verbs in present tense etc.\n",
        "\n",
        "Let's do it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeVsyIKnG6hY",
        "colab_type": "text"
      },
      "source": [
        "#### Importing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvVE1rsxG8He",
        "colab_type": "code",
        "outputId": "fb9321f5-de3f-4dc5-ce7c-b1563f07bb42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import string"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTbe_OoM57p9",
        "colab_type": "text"
      },
      "source": [
        "We start with an empty dictionary as our index. As we scrape more articles later we will instead of starting with an empty index just update the already created index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqwBMsjv6bOr",
        "colab_type": "text"
      },
      "source": [
        "The index is organized as:\n",
        "```\n",
        "{\n",
        "  \"word1\": \\[ID1, ID2, ...],\n",
        "  \"word2\": \\[ID5, ID8, ...],\n",
        "  ...\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_tsSula7ONI",
        "colab_type": "text"
      },
      "source": [
        "We start with just a single article from our meta-data database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TBG-No068Cx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entry = meta_data.loc[0,:].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCSyZSzUBS9V",
        "colab_type": "code",
        "outputId": "3bfc5fbe-0456-4f48-aab2-63741172f71f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "entry"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ID                                                           1\n",
              "title        Data leak reveals how China 'brainwashes' Uigh...\n",
              "summary      Leaked documents show new evidence of China's ...\n",
              "link         https://www.bbc.co.uk/news/world-asia-china-50...\n",
              "published                                           24/11/2019\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWfZEQpG7oEk",
        "colab_type": "text"
      },
      "source": [
        "### Text preprocessing\n",
        "Turn title to lowercase, remove accents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nP_6t0iKh84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_string(text):\n",
        "  text = text.lower() #to lowercase\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation)) #strip punctuation\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvWZvb4s_e-L",
        "colab_type": "code",
        "outputId": "0f7b5f04-d9db-4777-8cbf-8f958cc8edb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "process_string(entry.title)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data leak reveals how china brainwashes uighurs in prison camps'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFXsuTQKGSU6",
        "colab_type": "text"
      },
      "source": [
        "Now, lemmatize, i.e. word normalization.\n",
        "\n",
        "This method requires some additional information about the words. We need to find the word category of each word, e.g. verb, noun etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtWfwUq5GriG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N8ZzKMLG0dV",
        "colab_type": "text"
      },
      "source": [
        "Test the function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGxXCIneHKZ7",
        "colab_type": "code",
        "outputId": "dc8b8b70-9718-407d-a511-03bc73b08707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print(\"Apple: {}\\n Run: {}\\n Happy: {}\" .format(get_wordnet_pos(\"apple\"), get_wordnet_pos(\"run\"), get_wordnet_pos(\"happy\")))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple: n\n",
            " Run: v\n",
            " Happy: a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFBmqVwOIBi_",
        "colab_type": "text"
      },
      "source": [
        "We also need to remove stopwords, i.e. words with low informational value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKRZSEhCI_cG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop = stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpgU0fANJCtz",
        "colab_type": "text"
      },
      "source": [
        "Now we'll iterate over all words in text, lemmatize and return the transformed string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q1Vnrm0IYTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lem = WordNetLemmatizer()\n",
        "\n",
        "def stop_lemmatize(doc):\n",
        "    tokens = nltk.word_tokenize(doc)\n",
        "    tmp = \"\"\n",
        "    for w in tokens:\n",
        "        if w not in stop:\n",
        "            tmp += lem.lemmatize(w, get_wordnet_pos(w)) + \" \"\n",
        "    return tmp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E0kf2AOIyZT",
        "colab_type": "code",
        "outputId": "c15323d0-44ba-4a40-ecf7-19d90f399d5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "stop_lemmatize(doc = entry.title)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Data leak reveals China 'brainwashes ' Uighurs prison camp \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HoG2PiXLzfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_string(text):\n",
        "  text = text.lower() #to lowercase\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation)) #strip punctuation\n",
        "  text = stop_lemmatize(text)\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIhgs5AjMIAQ",
        "colab_type": "code",
        "outputId": "05911a8c-4703-4b77-d05b-620337e9afd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "%time process_string(entry.title)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.25 ms, sys: 0 ns, total: 4.25 ms\n",
            "Wall time: 5.08 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data leak reveals china brainwashes uighur prison camp '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5ONtU65MRV5",
        "colab_type": "text"
      },
      "source": [
        "Now we apply the process_string function to all titles and summaries in our database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wucZF2ONMYJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meta_processed = meta_data.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-QeP2F9NlqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_df(df):\n",
        "  df['title'] = df['title'].apply(process_string)\n",
        "  df['summary'] = df['summary'].apply(process_string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f_GpqAHMztr",
        "colab_type": "code",
        "outputId": "166cc21d-98ad-4278-d767-e8d8ce098896",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "%time transform_df(meta_processed)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 88.9 ms, sys: 5.55 ms, total: 94.4 ms\n",
            "Wall time: 100 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAs0QsWoM9IQ",
        "colab_type": "code",
        "outputId": "0062deae-2be8-4785-d197-9bc3f996e43a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "meta_processed.head(5)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>data leak reveals china brainwashes uighur pri...</td>\n",
              "      <td>leak document show new evidence china systemat...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-asia-china-50...</td>\n",
              "      <td>24/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>hong kong election prodemocracy group make big...</td>\n",
              "      <td>opposition candidate attract support expense p...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-asia-china-50...</td>\n",
              "      <td>24/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>dr congo many dead plane crash home</td>\n",
              "      <td>least 27 people dead passenger plane hit resid...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-africa-50536220</td>\n",
              "      <td>24/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>flood cause chaos france italy</td>\n",
              "      <td>bridge collapse near italian city savona sever...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-europe-50536502</td>\n",
              "      <td>24/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>queen hind cargo ship carry 14000 sheep overtu...</td>\n",
              "      <td>operation rescue animal way ship capsize roman...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-europe-50538592</td>\n",
              "      <td>24/11/2019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ID  ...   published\n",
              "0  1  ...  24/11/2019\n",
              "1  2  ...  24/11/2019\n",
              "2  3  ...  24/11/2019\n",
              "3  4  ...  24/11/2019\n",
              "4  5  ...  24/11/2019\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-p1OltOONMV",
        "colab_type": "text"
      },
      "source": [
        "In practice, we won't be transforming the whole meta-data database since that would mean creating index from scratch after every crawler iteration. Instead we would use only subset of the database with only newly added articles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoeBvn0wPJCM",
        "colab_type": "text"
      },
      "source": [
        "Now we can iterate over all entries to create the index. We'll go step by step again before wrapping it all in one nice function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD-MeaW2W8qC",
        "colab_type": "text"
      },
      "source": [
        "Merge title and summary into one field and drop all columns except for ID as we don't need those anymore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa8k3MXlXDcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meta_processed['text'] = meta_processed['title'] + \" \" + meta_processed['summary']\n",
        "drop_cols = ['title', 'summary', 'published', 'link']\n",
        "meta_processed = meta_processed.drop(drop_cols, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIoqUWscY7-J",
        "colab_type": "code",
        "outputId": "50d8cb10-cb3e-4e24-dd5e-58734d91c30d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "meta_processed.head(5)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>data leak reveals china brainwashes uighur pri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>hong kong election prodemocracy group make big...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>dr congo many dead plane crash home  least 27 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>flood cause chaos france italy  bridge collaps...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>queen hind cargo ship carry 14000 sheep overtu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ID                                               text\n",
              "0  1  data leak reveals china brainwashes uighur pri...\n",
              "1  2  hong kong election prodemocracy group make big...\n",
              "2  3  dr congo many dead plane crash home  least 27 ...\n",
              "3  4  flood cause chaos france italy  bridge collaps...\n",
              "4  5  queen hind cargo ship carry 14000 sheep overtu..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N240s8Nj8mRD",
        "colab_type": "text"
      },
      "source": [
        "Add this part to a transform_df function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiino7VO8o7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_df(df):\n",
        "  df = df\n",
        "  df['title'] = df['title'].apply(process_string)\n",
        "  df['summary'] = df['summary'].apply(process_string)\n",
        "  df['text'] = df['title'] + \" \" + df['summary']\n",
        "  drop_cols = ['title', 'summary', 'published', 'link']\n",
        "  df = df.drop(drop_cols, axis=1)\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UMJEydsAeur",
        "colab_type": "text"
      },
      "source": [
        "### Build index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srGNuGhDZo3b",
        "colab_type": "text"
      },
      "source": [
        "Now we'll build index with just one entry."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8_Uw87eZtHh",
        "colab_type": "code",
        "outputId": "acb9ddce-29de-4d27-fbd7-1f686e86f9d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "entry = meta_processed.loc[0,:].copy()\n",
        "print(entry)\n",
        "index_test = {}"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ID                                                      1\n",
            "text    data leak reveals china brainwashes uighur pri...\n",
            "Name: 0, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAzw0E4cZ5f9",
        "colab_type": "text"
      },
      "source": [
        "Split the entry to single words and return list and save entry's ID as object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C1n-qMUZ0_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = entry.text.split()\n",
        "ID = entry.ID"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO0AQCZRalH9",
        "colab_type": "text"
      },
      "source": [
        "Each word in index' vocabulary is a dictionary key and has its own posting list with IDs. Let's construct one word vocabulary as example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHgo8mtSae96",
        "colab_type": "code",
        "outputId": "a047eec1-1089-4b48-d181-c90e07812732",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "word = words[0]\n",
        "sample = {word: [ID]}\n",
        "print(sample)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'data': [1]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWWaf60bbEPf",
        "colab_type": "text"
      },
      "source": [
        "Now we iterate over all words and if they aren't in the vocabulary yet we add them. Also for each word we append the entry ID to the posting list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwRfN3uObCur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word in words:\n",
        "  if word in index_test.keys():\n",
        "    if ID not in index_test[word]:\n",
        "      index_test[word].append(ID)\n",
        "  else:\n",
        "    index_test[word] = [ID]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0wlU3SnctOf",
        "colab_type": "code",
        "outputId": "525eb8f4-9c00-4ed8-8917-cca7026d7ef9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(index_test)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'data': [1], 'leak': [1], 'reveals': [1], 'china': [1], 'brainwashes': [1], 'uighur': [1], 'prison': [1], 'camp': [1], 'document': [1], 'show': [1], 'new': [1], 'evidence': [1], 'systematic': [1], 'brainwashing': [1], 'detainee': [1]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJYeIbiRdU7l",
        "colab_type": "text"
      },
      "source": [
        "Now this process can be repeated for all entries in the database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XO1iPcPdkRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def index_it(entry, index):\n",
        "  words = entry.text.split()\n",
        "  ID = entry.ID\n",
        "  for word in words:\n",
        "    if word in index.keys():\n",
        "      if ID not in index[word]:\n",
        "        index[word].append(ID)\n",
        "    else:\n",
        "      index[word] = [ID]\n",
        "  return index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL1z26mseByu",
        "colab_type": "code",
        "outputId": "5d92b929-fcf8-4eba-a260-0a9feda3e66a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "ind = index_it(entry=entry, index= {})\n",
        "print(ind)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'data': [1], 'leak': [1], 'reveals': [1], 'china': [1], 'brainwashes': [1], 'uighur': [1], 'prison': [1], 'camp': [1], 'document': [1], 'show': [1], 'new': [1], 'evidence': [1], 'systematic': [1], 'brainwashing': [1], 'detainee': [1]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPNcdzIZI-On",
        "colab_type": "text"
      },
      "source": [
        "Again we can iterate over all entries in the database with scraped articles, process them append to index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju8eHusneNaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def index_all(df, index):\n",
        "  for i in range(len(df)):\n",
        "    entry = df.loc[i,:]\n",
        "    index = index_it(entry = entry, index = index)\n",
        "  return index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgoKdHiGevlu",
        "colab_type": "code",
        "outputId": "ea736dc7-9ba1-43e6-a3b5-ffde6188db4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "index = index_all(meta_processed, index = {})\n",
        "len(index)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "376"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqj9PXDtJLgG",
        "colab_type": "text"
      },
      "source": [
        "Finally we wrap everything in one nice function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RR7asUo8BX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_index(df, index):\n",
        "    to_add = transform_df(df)\n",
        "    index = index_all(df = to_add, index = index)\n",
        "    return index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpZyJuBp8CyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = build_index(df = meta_data, index = {})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoZv54Ey8slh",
        "colab_type": "code",
        "outputId": "2fe28486-c1fa-4358-d1a5-b1f4d870ddaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "len(idx)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "376"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkmRha0xAPhd",
        "colab_type": "text"
      },
      "source": [
        "And for future use we save the index to json file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7bAh9pUARGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "with open('index.json', 'w') as fp:\n",
        "    json.dump(idx, fp, sort_keys=True, indent=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vbaEOXCAaET",
        "colab_type": "text"
      },
      "source": [
        "It can be of course opened again with following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beY4e7mCAbzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('index.json', 'r') as f:\n",
        "    data = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVI6tcJh9mzP",
        "colab_type": "text"
      },
      "source": [
        "# Bb Ranked retrieval\n",
        "  The user would probably prefer the more relevant pages to be displayed before those that are less relevant (hopefully they're at least a bit relevant).\n",
        "For our search engine to support such option we need to store some information about the scraped documents that could be later used for this purpose.\n",
        "We'll use averaged word2vec for this purpose. Word2Vec model is single hidden-layer neural network. The hidden layer is actually what is so useful about this model. Given a word the layer's activation gives a unique vector that word. For each document we can iterate over all words, extract their vectors and then by averaging obtain a document vector. \n",
        "  Compared to other methods averaged word2vec has multiple advantages. Unlike simpler methods such as bag-of-words, n-grams and tf-idf the size of the vectors is fixed. For example bag-of-words is also using vectors but the size of these vectors equals the number of unique words in the corpus. This means that the computational and storage requirements get larger as the corpus gets larger.\n",
        "  Averaged word2vec is also able to represent the documents on more abstract level than simpler methods and should therefore provide better method of ranking.\n",
        "  We're using word2vec rather than doc2vec because we can simply use pretrained word2vec model to compute the document vectors. Using doc2vec would mean training a neural network from scratch which requires computational power, time and rather large dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aogzj51-XIr",
        "colab_type": "text"
      },
      "source": [
        "Import and download pretrained word2vec model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzGvdfpM9rAr",
        "colab_type": "code",
        "outputId": "c7fa1507-6158-4ad4-ec54-f4823784118d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-24 19:49:56--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.169.229\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.169.229|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  35.8MB/s    in 45s     \n",
            "\n",
            "2019-11-24 19:50:42 (35.0 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5ICnU0Q-bfo",
        "colab_type": "text"
      },
      "source": [
        "Load word2vec model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYt1PwiB99RY",
        "colab_type": "code",
        "outputId": "a886e0e2-bbed-40b3-b7e3-9f9400352b5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "word2vec = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQuWTsqA-fXc",
        "colab_type": "text"
      },
      "source": [
        "Try getting vectors for all words in the text and averaging to get single vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI5z0s2u-z-B",
        "colab_type": "code",
        "outputId": "20ce07cf-389f-40c6-d036-84605e0ec89e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(words)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['data', 'leak', 'reveals', 'china', 'brainwashes', 'uighur', 'prison', 'camp', 'leak', 'document', 'show', 'new', 'evidence', 'china', 'systematic', 'brainwashing', 'uighur', 'detainee']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIQbeAyZ-2ra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def average_vectors(word2vec_model, doc):\n",
        "    # remove out-of-vocabulary words\n",
        "    doc = [word for word in doc if word in word2vec_model.vocab]\n",
        "    if len(doc) == 0:\n",
        "      return np.zeros(300)\n",
        "    else:\n",
        "      return np.mean(word2vec_model[doc], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbVzj05u_BBz",
        "colab_type": "code",
        "outputId": "112a1231-7d19-46ce-b9a3-f4ccf0fc02be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%time test_vec = average_vectors(word2vec, words)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 718 µs, sys: 45 µs, total: 763 µs\n",
            "Wall time: 554 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSg9chdM_sMc",
        "colab_type": "text"
      },
      "source": [
        "Now we can iterate over documents, compute their vectors and construct a document vectors database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDajpm9W_z0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_ranking(df):\n",
        "  corpus = df[['ID', 'text']].copy()\n",
        "  doc_vecs = {}\n",
        "  for i in range(len(corpus)):\n",
        "    row = corpus.loc[i,:]\n",
        "    text = row.text.split()\n",
        "    doc_vecs[row.ID]=average_vectors(word2vec, text)\n",
        "  doc_vecs = pd.DataFrame.from_dict(data=doc_vecs, orient=\"index\")\n",
        "  doc_vecs['ID'] = doc_vecs.index\n",
        "  return doc_vecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdV6EV7EB0Ie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_vecs = prepare_ranking(df=meta_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwLh0z5hAiGF",
        "colab_type": "text"
      },
      "source": [
        "## C. Query processor\n",
        "The final part of a search engine is a query processor which actually performs the search task. Given a query by user the processor should return list of relevant documents.\n",
        "There are multiple types of queries. We'll start with a simple \"google-ish\" query where assume the user looks for documents relevant to all words in the query. Therefore we transform the query to boolean by connecting all words with AND operator.\n",
        "\n",
        "First, the processor preprocesses the query the same way as the indexer preprocessed the text. In other words, we normalize the query to match the format of text in the index. Next, the query is parsed to single words. We look into index if these words are part of the vocabulary. If a word is in index we retrieve its posting list. Finally, we look for intersection of all retrieved posting lists. The result is list of document IDs that the user asked for.\n",
        "However, we need to return something more useful than just a list of IDs. Therefore,we retrieve the information stored about the documents in the meta-data database. Before printing the results we should also rank the documents. This ranking should be based on relevance to query.\n",
        "Optionally, the user may ask for news only from limited time window, e.g. published today or last week. So we need to filter the retrieved documents if this happens.\n",
        "\n",
        "-----\n",
        "To implement:\n",
        " - Boolean query\n",
        " - phrase matching\n",
        " -----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyNAejSKMEyx",
        "colab_type": "text"
      },
      "source": [
        "### Normalize query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPu2TdudD797",
        "colab_type": "text"
      },
      "source": [
        "Let's define an example query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JUyKhZvEKjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = \"Trump Ukraine China\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmMKVw28EXcW",
        "colab_type": "text"
      },
      "source": [
        "Now we use the \"process string\" function from used by indexer to normalize the query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZKoWbSvEWnp",
        "colab_type": "code",
        "outputId": "747bae09-6cdf-4847-fa40-57f503c208c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"User query: {}.\" .format(test))\n",
        "test_norm = process_string(test)\n",
        "print(\"Normalized query: {}.\" .format(test_norm))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User query: Trump Ukraine China.\n",
            "Normalized query: trump ukraine china .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg4x9bo6E2Nd",
        "colab_type": "text"
      },
      "source": [
        "Now we split the query into words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNL9FbheEiSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_split = test_norm.split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psMVdYaeFvSr",
        "colab_type": "text"
      },
      "source": [
        "And we wrap this in function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlFn4ChDF1Mh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_query(query):\n",
        "  norm = process_string(query)\n",
        "  return norm.split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UX4rtmWMKLX",
        "colab_type": "text"
      },
      "source": [
        "### Retrieve from index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3rrUZ4uFOZR",
        "colab_type": "text"
      },
      "source": [
        "And we iterate over the words, looking if they're in the index vocabulary. If so then we retrieve the associated posting list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceY2E-MzFLKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "retrieved = []\n",
        "for word in test_split:\n",
        "  if word in index.keys():\n",
        "    retrieved.append(index[word])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SEe_66IHyyk",
        "colab_type": "text"
      },
      "source": [
        "Now we look for the intersection of all posting lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0czw-uJUH2HD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lists_intersection(lists):\n",
        "  intersect = list(set.intersection(*map(set, lists)))\n",
        "  intersect.sort()\n",
        "  return intersect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckV3JFxxHEQv",
        "colab_type": "code",
        "outputId": "a1393cff-78d7-49d8-c45f-166d2f5a3511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result = lists_intersection(retrieved)\n",
        "print(result)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNF2rAIvJjkk",
        "colab_type": "text"
      },
      "source": [
        "Let's wrap this part in a function before proceeding to formatting the results. The additional if statement is for cases when there's nothing retrieved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CQ4fLj9JqBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def search_googleish(query, index=idx):\n",
        "  query_split = process_query(query)\n",
        "  retrieved = []\n",
        "  for word in query_split:\n",
        "    if word in index.keys():\n",
        "      retrieved.append(index[word])\n",
        "  if len(retrieved)>0:\n",
        "    result = lists_intersection(retrieved)\n",
        "  else:\n",
        "      result = [0]\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6jorxcKK0NQ",
        "colab_type": "code",
        "outputId": "9ad149e5-7c32-45ed-f9ed-56e99573a4c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result_IDs = search_googleish(\"Trump\", index)\n",
        "print(result_IDs)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[19]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2bZhASYLQMT",
        "colab_type": "text"
      },
      "source": [
        "-----\n",
        "\n",
        "*TO DO:\n",
        "If there's no document retrieved, try removing one term and looking for simplified query + tell user that such document doesn't include term X.*\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu8le1C1MCWi",
        "colab_type": "text"
      },
      "source": [
        "### Retrieve meta-data\n",
        "Now we need to connect the retrieved IDs with some useful information stored in database that we first use to refine the results and then to print nice result to user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeNE1jlWMdsz",
        "colab_type": "code",
        "outputId": "49d569c1-2f0f-4819-9a23-d148c0abd118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#in real setting we'll read the database from file here\n",
        "#meta = pd.read_csv(\"database.csv\")\n",
        "\n",
        "#this is our database\n",
        "meta = meta_data.drop(['text'], axis=1).copy()\n",
        "meta.head(5)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>data leak reveals china brainwashes uighur pri...</td>\n",
              "      <td>leak document show new evidence china systemat...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-asia-china-50...</td>\n",
              "      <td>24/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>hong kong election prodemocracy group make big...</td>\n",
              "      <td>opposition candidate attract support expense p...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-asia-china-50...</td>\n",
              "      <td>24/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>dr congo many dead plane crash home</td>\n",
              "      <td>least 27 people dead passenger plane hit resid...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-africa-50536220</td>\n",
              "      <td>24/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>flood cause chaos france italy</td>\n",
              "      <td>bridge collapse near italian city savona sever...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-europe-50536502</td>\n",
              "      <td>24/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>queen hind cargo ship carry 14000 sheep overtu...</td>\n",
              "      <td>operation rescue animal way ship capsize roman...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-europe-50538592</td>\n",
              "      <td>24/11/2019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ID  ...   published\n",
              "0  1  ...  24/11/2019\n",
              "1  2  ...  24/11/2019\n",
              "2  3  ...  24/11/2019\n",
              "3  4  ...  24/11/2019\n",
              "4  5  ...  24/11/2019\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C8jU3h7Nf3L",
        "colab_type": "text"
      },
      "source": [
        "Query from database to get only rows of retrieved IDs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkZiljCvQDdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def connect_id_df(retrieved_id, df):\n",
        "    return df[df.ID.isin(retrieved_id)].reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAAm0Nc-M5Bh",
        "colab_type": "code",
        "outputId": "4c265ed5-2cdf-4b85-e7e2-ceb2563afdb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "result_meta = connect_id_df(result_IDs, meta)\n",
        "result_meta.head(5)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19</td>\n",
              "      <td>week impeachment news three minute</td>\n",
              "      <td>congress heard explosive testimony president d...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-us-canada-505...</td>\n",
              "      <td>22/11/2019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID  ...   published\n",
              "0  19  ...  22/11/2019\n",
              "\n",
              "[1 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twSUGf8jMahf",
        "colab_type": "text"
      },
      "source": [
        "### Ranked retrieval\n",
        "Now we return back to the word2vec vectors we computed after indexing the documents.\n",
        "We'l compute the vector for the query as well and then using a cosine similarity compare query to retrieved document relevance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LQ_hgGiM5ru",
        "colab_type": "text"
      },
      "source": [
        "Compute vector for query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-zOxFRfX5j3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query_vec = average_vectors(word2vec, test_split)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK6PYd3_PyOW",
        "colab_type": "text"
      },
      "source": [
        "Retrieve vectors of retrieve documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-EA-K5_LKxP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_vecs = connect_id_df(result_IDs, doc_vecs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZNPFm1iLtoo",
        "colab_type": "text"
      },
      "source": [
        "Compute cosine similarity between retrieved documents and query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xBh7uO3Ppu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cos_similarity(a, b):\n",
        "  dot = np.dot(a, b)\n",
        "  norma = np.linalg.norm(a)\n",
        "  normb = np.linalg.norm(b)\n",
        "  cos = dot / (norma * normb)\n",
        "  return(cos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUzOWptuNOR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cos_sim = []\n",
        "for i in range(len(result_vecs)):\n",
        "  doc_vec = result_vecs.loc[i,:].drop(['ID'])\n",
        "  cos_sim.append(cos_similarity(doc_vec, query_vec))\n",
        "result_meta['rank'] = cos_sim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oyuxNuTOTsw",
        "colab_type": "text"
      },
      "source": [
        "Sort retrieved docs by cosine similarity which is proxi for relevance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrdUUDdLMWax",
        "colab_type": "code",
        "outputId": "e3912c15-2ad4-40f5-c35b-8b600fe7099b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "result_meta.sort_values('rank', axis=0)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "      <th>rank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19</td>\n",
              "      <td>week impeachment news three minute</td>\n",
              "      <td>congress heard explosive testimony president d...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-us-canada-505...</td>\n",
              "      <td>22/11/2019</td>\n",
              "      <td>0.276263</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID                                title  ...   published      rank\n",
              "0  19  week impeachment news three minute   ...  22/11/2019  0.276263\n",
              "\n",
              "[1 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A0UidLgO4L4",
        "colab_type": "text"
      },
      "source": [
        "Wrap this in function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo8zJsy4N-i9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rank_results(query, results):\n",
        "  query_norm = process_query(query)\n",
        "  query_vec = average_vectors(word2vec, query_norm)\n",
        "  result_vecs = connect_id_df(results.ID, doc_vecs)\n",
        "  cos_sim = []\n",
        "  for i in range(len(result_vecs)):\n",
        "    doc_vec = result_vecs.loc[i,:].drop(['ID'])\n",
        "    cos_sim.append(cos_similarity(doc_vec, query_vec))\n",
        "  results['rank'] = cos_sim\n",
        "  results = results.sort_values('rank', axis=0)\n",
        "  return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTEOQ9pGQsbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_result = rank_results(\"Trump\", result_meta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0p1E-AgQymE",
        "colab_type": "text"
      },
      "source": [
        "### Date filtering\n",
        "User might ask for news from specific day or date range.\n",
        "For simplicity let's assume the user enters date in format day/month/year. \n",
        "Now we can define 2 types of date restrictions:\n",
        "* single day\n",
        "* date range - from X/X/X - to X/X/X\n",
        "\n",
        "Another option of date restriction could be saying 'today'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewBQDK20Vr_r",
        "colab_type": "text"
      },
      "source": [
        "Restricing to single day"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms7vreKCVw-O",
        "colab_type": "code",
        "outputId": "5675177c-9660-4666-ef4e-47fe4f2d52c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        }
      },
      "source": [
        "test = \"24/11/2019\"\n",
        "\n",
        "#get news published on \"test\"\n",
        "results_single = result_meta[result_meta.published==test].reset_index(drop=True)\n",
        "results_single.head()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "      <th>rank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [ID, title, summary, link, published, rank]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8menLWWZWYkJ",
        "colab_type": "text"
      },
      "source": [
        "Restricing to \"today\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgQDX7UGWM5S",
        "colab_type": "code",
        "outputId": "8fd1a959-590f-420c-eac5-fb5aaf72eedb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        }
      },
      "source": [
        "#get today's date\n",
        "from datetime import date, timedelta\n",
        "\n",
        "def get_today():\n",
        "  today = date.today()\n",
        "  today = today.strftime(\"%d/%m/%Y\")\n",
        "  return [today]\n",
        "\n",
        "results_today = result_meta[result_meta.published.isin(get_today())].reset_index(drop=True)\n",
        "results_today.head()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "      <th>rank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [ID, title, summary, link, published, rank]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pyy9xSXbcuC0",
        "colab_type": "code",
        "outputId": "f5723d07-8fae-4e50-88b4-cc5a0475a578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "get_today()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['24/11/2019']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-2VaQl2W_tW",
        "colab_type": "text"
      },
      "source": [
        "Restricting to time interval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkDZvU5QWyoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def daterange(start, end):\n",
        "    for n in range(int ((end - start).days)+1):\n",
        "        yield start + timedelta(n)\n",
        "\n",
        "def format_date(dt):\n",
        "  dt = dt.split(\"/\")\n",
        "  dt = date(int(dt[2]), int(dt[1]), int(dt[0]))\n",
        "  return(dt)\n",
        "\n",
        "def date_interval(interval):\n",
        "  interval = interval.split(\"-\")\n",
        "  start = format_date(interval[0])\n",
        "  end = format_date(interval[1])\n",
        "  interval = []\n",
        "  for dt in daterange(start, end):\n",
        "      interval.append(dt.strftime(\"%d/%m/%Y\"))\n",
        "  return interval"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0x2ZHXJXlYD",
        "colab_type": "code",
        "outputId": "d7c27fce-acd5-4085-b598-fa3b39af7d81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "date_interval(\"15/11/2019 - 16/11/2019\")"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['15/11/2019', '16/11/2019']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHpOEkPtDxv3",
        "colab_type": "code",
        "outputId": "65052791-21b5-4bdd-a8ff-c49691dabab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "s = \"16/11/2019\"\n",
        "len(s)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3z1_uUJ8IX5",
        "colab_type": "text"
      },
      "source": [
        "We also need to create the format in which the user should tell the search engine about the date restriction.\n",
        "Let's pass such request as a new argument for the search function. We can either then simply ask for two separate inputs from the user or find a more elegant solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O7Wv-5O8k73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_date(dat, df):\n",
        "  if dat == \"today\":\n",
        "    dat = get_today()\n",
        "  if len(dat) == 10:\n",
        "    dat = [dat]\n",
        "  if len(dat) > 11:\n",
        "    dat = date_interval(dat)\n",
        "  if len(dat) is 0:\n",
        "    return(df)\n",
        "\n",
        "  result = df[df.published.isin(dat)].reset_index(drop=True)\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vsh6-cMA_h2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filter_date(\"today\", )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsqjtFGcRf4j",
        "colab_type": "text"
      },
      "source": [
        "### Print results to user"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOz-Re8pQ0Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_results(result_df):\n",
        "  for i in range(len(result_df)):\n",
        "    res = result_df.loc[i, :]\n",
        "    print(res.title)\n",
        "    print(res.summary)\n",
        "    if i == len(result_df):\n",
        "        print(res.link)\n",
        "    else:\n",
        "        print(\"{}\\n\" .format(res.link))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yY8sTZ-SEFb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6496f4ca-d7f6-45f9-ea26-093113ebe205"
      },
      "source": [
        "print_results(final_result)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "week impeachment news three minute \n",
            "congress heard explosive testimony president donald trump impeachment inquiry \n",
            "https://www.bbc.co.uk/news/world-us-canada-50505712\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXZadNiKUIGK",
        "colab_type": "text"
      },
      "source": [
        "### Put it all together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdm4KHFsUKGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def search(query, dat=None):\n",
        "  result = search_googleish(query)\n",
        "  result = connect_id_df(result, meta)\n",
        "  result = rank_results(query, result)\n",
        "\n",
        "  if dat is not None:\n",
        "    result = filter_date(dat, result)\n",
        "\n",
        "  print_results(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yVCIapYUMpJ",
        "colab_type": "code",
        "outputId": "1832a855-08f1-40a2-b012-fab1b2fce7e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "query = input(\"Search for:\")\n",
        "dat = input(\"Date:\")\n",
        "search(query, dat)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Search for:data china leak\n",
            "Date:\n",
            "data leak reveals china brainwashes uighur prison camp \n",
            "leak document show new evidence china systematic brainwashing uighur detainee \n",
            "https://www.bbc.co.uk/news/world-asia-china-50511063\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06lpGwT1xDGm",
        "colab_type": "text"
      },
      "source": [
        "### Automated crawling every hour\n",
        "Now we want the crawler to scrape new articles every hour by itself. Probably the simplest and most pythonic solution is a while loop and time.sleep().\n",
        "\n",
        "First we create a list of feeds that we want to scrape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umhbbqprxWYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "URLS = ['http://feeds.bbci.co.uk/news/world/rss.xml', \n",
        "        'http://feeds.bbci.co.uk/news/uk/rss.xml', \n",
        "        'http://www.independent.co.uk/news/uk/rss', \n",
        "        'feed:https://rss.nytimes.com/services/xml/rss/nyt/World.xml', \n",
        "        'feed:https://rss.nytimes.com/services/xml/rss/nyt/US.xml', \n",
        "        'feed://feeds.washingtonpost.com/rss/world?tid=lk_inline_manual_13']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llPsuGpXxdnh",
        "colab_type": "text"
      },
      "source": [
        "Now we iterate over these urls, scrape data and update the index. NOTE that this is a pseudo-code, the function definitions are stored in their respective scripts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pu8pbZkxkk7",
        "colab_type": "code",
        "outputId": "91eea35c-2d1f-41b1-b75d-f1b86d94354f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "for url in URLS:\n",
        "  print (\"Crawling {}\" .format(url))\n",
        "  #perc, added = crawl(URL=url, PATH=OUTPUT_DIR)\n",
        "  print(\"XXX % of entries were already scraped.\\n\")\n",
        "  #update_index_vecs(df=added, index_path=INDEX_PATH, vec_path=VECTOR_PATH)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Crawling http://feeds.bbci.co.uk/news/world/rss.xml\n",
            "XXX % of entries were already scraped.\n",
            "\n",
            "Crawling http://feeds.bbci.co.uk/news/uk/rss.xml\n",
            "XXX % of entries were already scraped.\n",
            "\n",
            "Crawling http://www.independent.co.uk/news/uk/rss\n",
            "XXX % of entries were already scraped.\n",
            "\n",
            "Crawling feed:https://rss.nytimes.com/services/xml/rss/nyt/World.xml\n",
            "XXX % of entries were already scraped.\n",
            "\n",
            "Crawling feed:https://rss.nytimes.com/services/xml/rss/nyt/US.xml\n",
            "XXX % of entries were already scraped.\n",
            "\n",
            "Crawling feed://feeds.washingtonpost.com/rss/world?tid=lk_inline_manual_13\n",
            "XXX % of entries were already scraped.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY6aPt74yMx8",
        "colab_type": "text"
      },
      "source": [
        "And we put this loop in a while loop. After the for loop is finished we pause the code execution for an hour. Then the code starts from the top again. \n",
        "Such while loop would run forever (literally, don't try!!!) so I'm leaving it commented out here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bC-PpShylIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "#while True:\n",
        "#  for url in URLS:\n",
        "#   print (\"Crawling {}\" .format(url))\n",
        "    #perc, added = crawl(URL=url, PATH=OUTPUT_DIR)\n",
        "#   print(\"XXX % of entries were already scraped.\\n\")\n",
        "    #update_index_vecs(df=added, index_path=INDEX_PATH, vec_path=VECTOR_PATH)\n",
        "\n",
        "  #pause for an hour\n",
        "#  time.sleep(3600)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO8XaAcOy3yU",
        "colab_type": "text"
      },
      "source": [
        "In real setting you'll need to find some restriction for this while loop. For example let it run for a day can be done as follows (also not recommended to run)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZy3aXqZzLlw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "4221e536-fc7f-4d35-bffd-bc51152d6f42"
      },
      "source": [
        "hours = 1\n",
        "\n",
        "while hours < 24:\n",
        "#  for url in URLS:\n",
        "#   print (\"Crawling {}\" .format(url))\n",
        "  # perc, added = crawl(URL=url, PATH=OUTPUT_DIR)\n",
        "#   print(\"XXX % of entries were already scraped.\\n\")\n",
        "  # update_index_vecs(df=added, index_path=INDEX_PATH, vec_path=VECTOR_PATH)\n",
        "\n",
        "  #pause for 2 seconds - normally this would be 3600 for waiting an hour\n",
        "   print(hours)\n",
        "   time.sleep(2)\n",
        "   hours +=1"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}