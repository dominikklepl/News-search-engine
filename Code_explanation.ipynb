{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Code explanation.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dominikklepl/News-search-engine/blob/master/Code_explanation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkJu3K5k8tWk",
        "colab_type": "text"
      },
      "source": [
        "# News search engine\n",
        "In this notebook we'll go through all code for building our own search engine specialised on news articles.\n",
        "Each search engine consists of three main components:\n",
        "\n",
        "\n",
        "* Crawler\n",
        "* Indexer\n",
        "* Query processor\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG-1-CN_mLTu",
        "colab_type": "text"
      },
      "source": [
        "## A. Crawler\n",
        "We start with building crawler that goes to RSS feed, extracts title, description (if any), date published and link. Processes this information and saves in a meta-data \"database\" (actually a pandas dataframe and stores it as csv file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfRHdlHTmLTx",
        "colab_type": "text"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKMcZm97mLTy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0b345e6-a4c2-42cb-8761-d7b26321540c"
      },
      "source": [
        "!pip install feedparser\n",
        "\n",
        "import feedparser\n",
        "import pandas as pd"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.6/dist-packages (5.2.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99djxLQM2GJL",
        "colab_type": "text"
      },
      "source": [
        "Read RSS feed from a URL. Also keep track of how many news are in the feed and how many of these have already been scraped with previous iteration of the crawler. This will be later important to automatically adjust the frequency with which the crawler will visit the RSS feed to scrape new information.\n",
        "For testing we use just one URL: BBC World News."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGjcps0s2rc4",
        "colab_type": "code",
        "outputId": "68316058-35f4-4fe0-fc5a-3757f3e72a6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "URL = \"http://feeds.bbci.co.uk/news/world/rss.xml\"\n",
        "feed = feedparser.parse(URL)\n",
        "\n",
        "feed_len = len(feed.entries) #number of news in feed\n",
        "old_news = 0  # count how many news in feed were already scraped\n",
        "\n",
        "print(\"There are {} news in the RSS feed.\" .format(feed_len))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 38 news in the RSS feed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27WxoGCj2vz3",
        "colab_type": "text"
      },
      "source": [
        "Load the meta-data database stored as csv file. If this is the first time the crawler is let loose this will be just an empty file with prepared column names (ID, title, date and link).\n",
        "I'll keep it commented out here and instead just create an empty dataframe a this point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6VVIE0W3FLm",
        "colab_type": "code",
        "outputId": "0ce1ac90-d514-4ba3-c43b-101c3fc1dc86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        }
      },
      "source": [
        "#meta_data = pd.read_csv(PATH + \"database.csv\", index_col = 'Unnamed: 0')\n",
        "meta_data = pd.DataFrame(columns=['ID', 'title', 'summary', 'link', 'published'])\n",
        "meta_data.head()"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [ID, title, summary, link, published]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZrsrQxM4ADx",
        "colab_type": "text"
      },
      "source": [
        "Now we write a function that accepts one entry from the feed and parse its contents to list with title, date published and link and assigns it a unique ID (which also denotes when was the entry scraped and entered to our search engine).\n",
        "We normalize the date so that it's in format day/month/year. We don't care about more precise time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLs86-jb4Ygm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_entry(entry, ID):\n",
        "  ID = ID\n",
        "  title = entry.title\n",
        "  summary = entry.summary\n",
        "  link = entry.link\n",
        "  published = str(entry.published_parsed.tm_mday) + '/' + \\\n",
        "              str(entry.published_parsed.tm_mon) + '/' + \\\n",
        "              str(entry.published_parsed.tm_year)\n",
        "  return [ID, title, summary, link, published]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ6kXhpj4dPY",
        "colab_type": "text"
      },
      "source": [
        "Test the function on one entry"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6oLq8av4fSV",
        "colab_type": "code",
        "outputId": "39c28bac-14a2-450b-8c23-7860bcb2b58d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "test_entry = feed.entries[0]\n",
        "process_entry(test_entry, 1)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " \"Hong Kong protests: China condemns 'appalling' attack on official in UK\",\n",
              " \"Hong Kong's Justice Secretary Teresa Cheng was surrounded by protesters in London.\",\n",
              " 'https://www.bbc.co.uk/news/world-asia-china-50433799',\n",
              " '15/11/2019']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMUwB0-u6xd3",
        "colab_type": "text"
      },
      "source": [
        "Now we iterate over all entries in the feed. Check if the entry is already in the meta-data database, if not the entry is processed, assigned an ID and appended to a list of entries that will be later added to the database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3_zLbxk7FQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [] #dataframe for saving the entries\n",
        "n = len(meta_data)+1 #ID value based on the highest ID value in database\n",
        "for i in range(len(feed.entries)):\n",
        "  entry = feed.entries[i]\n",
        "  \n",
        "  #check that link isn't in the database yet\n",
        "  if entry.link not in meta_data['link'].values:\n",
        "    processed = process_entry(entry = entry, ID=n)\n",
        "    data.append(processed)\n",
        "    n += 1 #increase the ID value\n",
        "  else: old_news += 1 #count already scraped entries"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbOf3R417oQJ",
        "colab_type": "text"
      },
      "source": [
        "If there was at least one newly scraped entry, we add it to the database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6mtKUT_7ypt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if len(data) > 0:\n",
        "  #transform data to pandas DataFrame\n",
        "  news_extracted = pd.DataFrame(data, columns=['ID', 'title', 'summary', 'link', 'published'])\n",
        "\n",
        "  #add new news to the database\n",
        "  meta_data = pd.concat([meta_data, news_extracted], axis = 0)\n",
        "\n",
        "  #write database to a csv file\n",
        "  #meta_data.to_csv(PATH + \"database.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRErvxw48RGs",
        "colab_type": "text"
      },
      "source": [
        "Look at the database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtZVgM-98Szw",
        "colab_type": "code",
        "outputId": "52a36796-4e9a-49cc-81a6-7507bad52c6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "meta_data.head(5)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Hong Kong protests: China condemns 'appalling'...</td>\n",
              "      <td>Hong Kong's Justice Secretary Teresa Cheng was...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-asia-china-50...</td>\n",
              "      <td>15/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Flooded Venice battles new tidal surge</td>\n",
              "      <td>The Italian canal city's main square, waterbus...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-europe-50430855</td>\n",
              "      <td>15/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Chile protests: Government bows to demands for...</td>\n",
              "      <td>After weeks of unrest in the country, Chile ha...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-latin-america...</td>\n",
              "      <td>15/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Spy swap: Five freed in Russia-Lithuania-Norwa...</td>\n",
              "      <td>Russia takes part in a carefully co-ordinated ...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-europe-50431713</td>\n",
              "      <td>15/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Ultra-long haul flight non-stop from London to...</td>\n",
              "      <td>The BBC's Luke Jones was on-board the 19 and a...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-australia-504...</td>\n",
              "      <td>15/11/2019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ID  ...   published\n",
              "0  1  ...  15/11/2019\n",
              "1  2  ...  15/11/2019\n",
              "2  3  ...  15/11/2019\n",
              "3  4  ...  15/11/2019\n",
              "4  5  ...  15/11/2019\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8IWAaU19VQ-",
        "colab_type": "text"
      },
      "source": [
        "Get percentage of already-scraped entries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gub9dSMX9aVy",
        "colab_type": "code",
        "outputId": "9b77510e-3915-4bf5-d984-762ec7aa4045",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"{} % of entries were already scraped.\" .format((old_news/feed_len)*100))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100.0 % of entries were already scraped.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2yM7uUb3yqT",
        "colab_type": "text"
      },
      "source": [
        "## B. Indexer\n",
        "Second part of a search engine is an indexer. It's basically a smart storage of our news articles in which we can later easily retrieve relative articles given a search query.\n",
        "It parses the title and description of the news articles scraped by the crawler to single words. All these words make up the vocabulary of our index. Next step is to put the ID of the article in the posting lists of the words that the article contains. For example article called \"This happened today\" will be stored in posting lists of terms \"this\", \"happened\" and \"today\".\n",
        "Before creating the index we preprocess the text of the articles in order to get rid of useless information. We the text of accents and turn everything to lowercase. Next we perform lemmatization. This is slightly smarter version of stemming. Essentially, it's a word normalization, e.g. all nouns to singular, all verbs in present tense etc.\n",
        "\n",
        "Let's do it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeVsyIKnG6hY",
        "colab_type": "text"
      },
      "source": [
        "#### Importing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvVE1rsxG8He",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "b5ff708d-1bb7-4b87-acf8-e763e3fca412"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import string"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTbe_OoM57p9",
        "colab_type": "text"
      },
      "source": [
        "We start with an empty dictionary as our index. As we scrape more articles later we will instead of starting with an empty index just update the already created index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3H70L_x56Hg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqwBMsjv6bOr",
        "colab_type": "text"
      },
      "source": [
        "The index is organized as:\n",
        "```\n",
        "{\n",
        "  \"word1\": \\[ID1, ID2, ...],\n",
        "  \"word2\": \\[ID5, ID8, ...],\n",
        "  ...\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_tsSula7ONI",
        "colab_type": "text"
      },
      "source": [
        "We start with just a single article from our meta-data database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TBG-No068Cx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entry = meta_data.loc[0,:].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCSyZSzUBS9V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "60f91797-2157-4ea1-d258-b8e0467a2155"
      },
      "source": [
        "entry"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ID                                                           1\n",
              "title        Hong Kong protests: China condemns 'appalling'...\n",
              "summary      Hong Kong's Justice Secretary Teresa Cheng was...\n",
              "link         https://www.bbc.co.uk/news/world-asia-china-50...\n",
              "published                                           15/11/2019\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWfZEQpG7oEk",
        "colab_type": "text"
      },
      "source": [
        "### Text preprocessing\n",
        "Turn title to lowercase, remove accents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nP_6t0iKh84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_string(text):\n",
        "  text = text.lower() #to lowercase\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation)) #strip punctuation\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvWZvb4s_e-L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "09fd7681-65a0-4e62-d1ed-c113da11a986"
      },
      "source": [
        "process_string(entry.title)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hong kong protests china condemns appalling attack on official in uk'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFXsuTQKGSU6",
        "colab_type": "text"
      },
      "source": [
        "Now, lemmatize, i.e. word normalization.\n",
        "\n",
        "This method requires some additional information about the words. We need to find the word category of each word, e.g. verb, noun etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtWfwUq5GriG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N8ZzKMLG0dV",
        "colab_type": "text"
      },
      "source": [
        "Test the function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGxXCIneHKZ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2d0ec63d-bd44-4348-8f9b-bd754e2b548d"
      },
      "source": [
        "print(\"Apple: {}\\n Run: {}\\n Happy: {}\" .format(get_wordnet_pos(\"apple\"), get_wordnet_pos(\"run\"), get_wordnet_pos(\"happy\")))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple: n\n",
            " Run: v\n",
            " Happy: a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFBmqVwOIBi_",
        "colab_type": "text"
      },
      "source": [
        "We also need to remove stopwords, i.e. words with low informational value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKRZSEhCI_cG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop = stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpgU0fANJCtz",
        "colab_type": "text"
      },
      "source": [
        "Now we'll iterate over all words in text, lemmatize and return the transformed string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q1Vnrm0IYTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lem = WordNetLemmatizer()\n",
        "\n",
        "def stop_lemmatize(doc):\n",
        "    tokens = nltk.word_tokenize(doc)\n",
        "    tmp = \"\"\n",
        "    for w in tokens:\n",
        "        if w not in stop:\n",
        "            tmp += lem.lemmatize(w, get_wordnet_pos(w)) + \" \"\n",
        "    return tmp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E0kf2AOIyZT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa03d1eb-76bd-4042-a57a-6d2e70a82a51"
      },
      "source": [
        "stop_lemmatize(doc = entry.title)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hong Kong protest : China condemns 'appalling ' attack official UK \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HoG2PiXLzfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_string(text):\n",
        "  text = text.lower() #to lowercase\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation)) #strip punctuation\n",
        "  text = stop_lemmatize(text)\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIhgs5AjMIAQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "cfa28072-d0be-44cc-d1cd-03896b249568"
      },
      "source": [
        "%time process_string(entry.title)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3.02 ms, sys: 2.78 ms, total: 5.8 ms\n",
            "Wall time: 11.9 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hong kong protest china condemns appal attack official uk '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5ONtU65MRV5",
        "colab_type": "text"
      },
      "source": [
        "Now we apply the process_string function to all titles and summaries in our database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wucZF2ONMYJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meta_processed = meta_data.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-QeP2F9NlqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_df(df):\n",
        "  df['title'] = df['title'].apply(process_string)\n",
        "  df['summary'] = df['summary'].apply(process_string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f_GpqAHMztr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e3131370-fc0e-4c23-afe8-7a04a227bcab"
      },
      "source": [
        "%time transform_df(meta_processed)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 126 ms, sys: 6.06 ms, total: 132 ms\n",
            "Wall time: 141 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAs0QsWoM9IQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bdbc78a9-2168-47e9-8e96-e60b7125293c"
      },
      "source": [
        "meta_processed.head(5)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>hong kong protest china condemns appal attack ...</td>\n",
              "      <td>hong kongs justice secretary teresa cheng surr...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-asia-china-50...</td>\n",
              "      <td>15/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>flood venice battle new tidal surge</td>\n",
              "      <td>italian canal city main square waterbuses scho...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-europe-50430855</td>\n",
              "      <td>15/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>chile protest government bow demand referendum</td>\n",
              "      <td>week unrest country chile agree hold referendu...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-latin-america...</td>\n",
              "      <td>15/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>spy swap five freed russialithuanianorway exch...</td>\n",
              "      <td>russia take part carefully coordinate exchange...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-europe-50431713</td>\n",
              "      <td>15/11/2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>ultralong haul flight nonstop london sydney</td>\n",
              "      <td>bbcs luke jones onboard 19 half hour flight lo...</td>\n",
              "      <td>https://www.bbc.co.uk/news/world-australia-504...</td>\n",
              "      <td>15/11/2019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ID  ...   published\n",
              "0  1  ...  15/11/2019\n",
              "1  2  ...  15/11/2019\n",
              "2  3  ...  15/11/2019\n",
              "3  4  ...  15/11/2019\n",
              "4  5  ...  15/11/2019\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-p1OltOONMV",
        "colab_type": "text"
      },
      "source": [
        "In practice, we won't be transforming the whole meta-data database since that would mean creating index from scratch after every crawler iteration. Instead we would use only subset of the database with only newly added articles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoeBvn0wPJCM",
        "colab_type": "text"
      },
      "source": [
        "Now we can iterate over all entries to create the index. We'll go step by step again before wrapping it all in one nice function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD-MeaW2W8qC",
        "colab_type": "text"
      },
      "source": [
        "Merge title and summary into one field and drop all columns except for ID as we don't need those anymore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa8k3MXlXDcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meta_processed['text'] = meta_processed['title'] + \" \" + meta_processed['summary']\n",
        "drop_cols = ['title', 'summary', 'published', 'link']\n",
        "meta_processed = meta_processed.drop(drop_cols, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIoqUWscY7-J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d72bcf4c-a2be-4b32-8491-a5202621bfe6"
      },
      "source": [
        "meta_processed.head(5)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>hong kong protest china condemns appal attack ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>flood venice battle new tidal surge  italian c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>chile protest government bow demand referendum...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>spy swap five freed russialithuanianorway exch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>ultralong haul flight nonstop london sydney  b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ID                                               text\n",
              "0  1  hong kong protest china condemns appal attack ...\n",
              "1  2  flood venice battle new tidal surge  italian c...\n",
              "2  3  chile protest government bow demand referendum...\n",
              "3  4  spy swap five freed russialithuanianorway exch...\n",
              "4  5  ultralong haul flight nonstop london sydney  b..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srGNuGhDZo3b",
        "colab_type": "text"
      },
      "source": [
        "Now we'll build index with just one entry."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8_Uw87eZtHh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3d8e15ca-b87f-4353-becd-8366bd09a346"
      },
      "source": [
        "entry = meta_processed.loc[0,:].copy()\n",
        "print(entry)\n",
        "index_test = {}"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ID                                                      1\n",
            "text    hong kong protest china condemns appal attack ...\n",
            "Name: 0, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAzw0E4cZ5f9",
        "colab_type": "text"
      },
      "source": [
        "Split the entry to single words and return list and save entry's ID as object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C1n-qMUZ0_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = entry.text.split()\n",
        "ID = entry.ID"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO0AQCZRalH9",
        "colab_type": "text"
      },
      "source": [
        "Each word in index' vocabulary is a dictionary key and has its own posting list with IDs. Let's construct one word vocabulary as example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHgo8mtSae96",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "574136b6-cd5f-46f2-d5bf-391b20a309e4"
      },
      "source": [
        "word = words[0]\n",
        "sample = {word: [ID]}\n",
        "print(sample)"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'hong': [1]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWWaf60bbEPf",
        "colab_type": "text"
      },
      "source": [
        "Now we iterate over all words and if they aren't in the vocabulary yet we add them. Also for each word we append the entry ID to the posting list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwRfN3uObCur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word in words:\n",
        "  if word in index_test.keys():\n",
        "    if ID not in index_test[word]:\n",
        "      index_test[word].append(ID)\n",
        "  else:\n",
        "    index_test[word] = [ID]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0wlU3SnctOf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "99351cc9-4896-4d95-df91-94ebcc1da198"
      },
      "source": [
        "print(index_test)"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'hong': [1], 'kong': [1], 'protest': [1], 'china': [1], 'condemns': [1], 'appal': [1], 'attack': [1], 'official': [1], 'uk': [1], 'kongs': [1], 'justice': [1], 'secretary': [1], 'teresa': [1], 'cheng': [1], 'surround': [1], 'protester': [1], 'london': [1]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJYeIbiRdU7l",
        "colab_type": "text"
      },
      "source": [
        "Now this process can be repeated for all entries in the database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XO1iPcPdkRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_index(entry, index):\n",
        "  words = entry.text.split()\n",
        "  ID = entry.ID\n",
        "\n",
        "  for word in words:\n",
        "    if word in index.keys():\n",
        "      if ID not in index[word]:\n",
        "        index[word].append(ID)\n",
        "    else:\n",
        "      index[word] = [ID]\n",
        "  return index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL1z26mseByu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ind = update_index(entry=entry, index= {})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju8eHusneNaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = {}\n",
        "for i in range(len(meta_processed)):\n",
        "  entry = meta_processed.loc[i,:]\n",
        "  index = update_index(entry = entry, index = index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgoKdHiGevlu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "18e72727-c1e7-47b5-b8fe-50b86ea564c8"
      },
      "source": [
        "len(index)"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "440"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    }
  ]
}